{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Said20038/DeepLearning/blob/main/TP%20Classification%20Multiclass%20RGB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "uNmBnEQC85y0",
        "outputId": "266146a3-a36a-4873-fc1a-4bddd99c9cc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.6.15)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
            "Bibliothèques importées.\n",
            "\n",
            "---\n",
            "Veuillez maintenant importer votre fichier 'kaggle.json'...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f0292b4d-76e0-4db0-82cf-f04fb824e522\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f0292b4d-76e0-4db0-82cf-f04fb824e522\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "'kaggle.json' importé avec succès !\n"
          ]
        }
      ],
      "source": [
        "# Installation de la bibliothèque Kaggle pour télécharger la base de données\n",
        "!pip install kaggle\n",
        "\n",
        "# Importation des bibliothèques nécessaires\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Bibliothèques importées.\")\n",
        "\n",
        "# --- Configuration de l'API Kaggle ---\n",
        "# Créez un dossier .kaggle et copiez-y votre fichier kaggle.json\n",
        "# Ce fichier contient vos identifiants API Kaggle.\n",
        "# Vous pouvez le télécharger depuis votre compte Kaggle > Settings > API > Create New Token\n",
        "print(\"\\n---\")\n",
        "print(\"Veuillez maintenant importer votre fichier 'kaggle.json'...\")\n",
        "if os.path.exists('kaggle.json'):\n",
        "    os.remove('kaggle.json')\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "if 'kaggle.json' in uploaded:\n",
        "    print(\"'kaggle.json' importé avec succès !\")\n",
        "    !mkdir -p ~/.kaggle\n",
        "    !cp kaggle.json ~/.kaggle/\n",
        "    !chmod 600 ~/.kaggle/kaggle.json\n",
        "else:\n",
        "    print(\"Erreur : Le fichier 'kaggle.json' n'a pas été importé.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlIqT21D9GSW",
        "outputId": "c8975103-fbb8-47d7-8426-6cc86806fbd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/benaddym/amazigh-handwritten-character-database-amhcd\n",
            "License(s): other\n",
            "Downloading amazigh-handwritten-character-database-amhcd.zip to /content\n",
            " 91% 62.0M/67.9M [00:00<00:00, 104MB/s]\n",
            "100% 67.9M/67.9M [00:00<00:00, 97.8MB/s]\n",
            "\n",
            "Base de données AMHCD téléchargée et décompressée.\n"
          ]
        }
      ],
      "source": [
        "# Téléchargement de la base de données AMHCD depuis Kaggle\n",
        "# Lien de la base de données : https://www.kaggle.com/datasets/benaddym/amazigh-handwritten-character-database-amhcd\n",
        "!kaggle datasets download -d benaddym/amazigh-handwritten-character-database-amhcd --unzip\n",
        "\n",
        "print(\"\\nBase de données AMHCD téléchargée et décompressée.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRwNvaxHUpHj",
        "outputId": "858ab610-fffd-4707-d766-33ddfcee0721"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete. Initial memory usage:\n",
            "Current memory usage: 774.03 MB\n"
          ]
        }
      ],
      "source": [
        "# Cell 1 - Setup and Imports\n",
        "!pip install psutil -q\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import psutil\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Function to print memory usage\n",
        "def print_memory_usage():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    mem = process.memory_info().rss / 1024 ** 2  # Memory in MB\n",
        "    print(f\"Current memory usage: {mem:.2f} MB\")\n",
        "\n",
        "print(\"Setup complete. Initial memory usage:\")\n",
        "print_memory_usage()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIbiwlMjUsv_",
        "outputId": "f4266d05-c0d0-4692-d8ed-a23fc75d17ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chargement des données...\n",
            "Recherche des données dans le dossier : 'AMHCD_64/AMHCD_64/'\n",
            "Trouvé 33 dossiers de classe. Exemple : ['ya', 'yab', 'yach']\n",
            "Data shape after loading: (25740, 1024), Memory usage after loading:\n",
            "Current memory usage: 905.08 MB\n",
            "\n",
            "Chargement des données réussi ! 25740 images chargées.\n"
          ]
        }
      ],
      "source": [
        "# Cell 2 - Data Loading Function\n",
        "def load_data(img_size=32):\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    # Define data path\n",
        "    data_path = 'AMHCD_64/AMHCD_64/'\n",
        "    print(f\"Recherche des données dans le dossier : '{data_path}'\")\n",
        "\n",
        "    if not os.path.isdir(data_path):\n",
        "        print(f\"ERREUR : Le chemin de base '{data_path}' n'a pas été trouvé.\")\n",
        "        return None, None\n",
        "\n",
        "    class_names = sorted(os.listdir(data_path))\n",
        "    print(f\"Trouvé {len(class_names)} dossiers de classe. Exemple : {class_names[:3]}\")\n",
        "\n",
        "    for label_index, class_name in enumerate(class_names):\n",
        "        class_path = os.path.join(data_path, class_name)\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "        for img_name in os.listdir(class_path):\n",
        "            try:\n",
        "                img_path = os.path.join(class_path, img_name)\n",
        "                img = cv2.imread(img_path)\n",
        "                if img is None:\n",
        "                    print(f\"Avertissement : Impossible de lire l'image {img_path}\")\n",
        "                    continue\n",
        "                img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "                img_resized = cv2.resize(img_gray, (img_size, img_size))\n",
        "                data.append(img_resized)\n",
        "                labels.append(label_index)\n",
        "            except Exception as e:\n",
        "                print(f\"Erreur de traitement de l'image {img_path}: {e}\")\n",
        "\n",
        "    data = np.array(data, dtype=\"float32\") / 255.0\n",
        "    data = data.reshape((data.shape[0], img_size * img_size))\n",
        "    labels = np.array(labels)\n",
        "    print(f\"Data shape after loading: {data.shape}, Memory usage after loading:\")\n",
        "    print_memory_usage()\n",
        "    return data, labels\n",
        "\n",
        "# Load data\n",
        "print(\"Chargement des données...\")\n",
        "X, y = load_data()\n",
        "\n",
        "if X is not None and y is not None and len(X) > 0:\n",
        "    print(f\"\\nChargement des données réussi ! {len(X)} images chargées.\")\n",
        "else:\n",
        "    print(\"\\nLe chargement des données a échoué. Veuillez vérifier les messages d'erreur ci-dessus.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "_KYpm1ksW0gx",
        "outputId": "02ded9a3-fb38-4123-9cc7-fc239df3bebf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels one-hot encoded. Shape: (25740, 33), Memory usage:\n",
            "Current memory usage: 905.08 MB\n",
            "Data augmented. New X shape: (51480, 1024), Memory usage:\n",
            "Current memory usage: 1288.41 MB\n",
            "Data standardized. Train Mean: -0.0002, Train Std: 1.0000, Memory usage:\n",
            "Current memory usage: 1390.05 MB\n",
            "\n",
            "Forme des données d'entraînement (X_train): (41184, 1024)\n",
            "Forme des labels d'entraînement (y_train): (41184, 33)\n",
            "Forme des données de test (X_test): (10296, 1024)\n",
            "Forme des labels de test (y_test): (10296, 33)\n",
            "Nombre total de classes: 33, Memory usage:\n",
            "Current memory usage: 1390.05 MB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 200x200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Cell 3 - Data Preprocessing and Splitting (Corrigée)\n",
        "if X is not None and y is not None and len(X) > 0:\n",
        "    # One-Hot Encoding\n",
        "    lb = LabelBinarizer()\n",
        "    y_onehot = lb.fit_transform(y)\n",
        "    print(f\"Labels one-hot encoded. Shape: {y_onehot.shape}, Memory usage:\")\n",
        "    print_memory_usage()\n",
        "\n",
        "    # Data augmentation\n",
        "    def augment_image(img, target_size=(32, 32)):\n",
        "        angle = np.random.uniform(-15, 15)\n",
        "        M = cv2.getRotationMatrix2D((16, 16), angle, 1)\n",
        "        img = cv2.warpAffine(img.reshape(32, 32), M, target_size)\n",
        "        tx, ty = np.random.uniform(-3, 3, 2)\n",
        "        M = np.float32([[1, 0, tx], [0, 1, ty]])\n",
        "        img = cv2.warpAffine(img, M, target_size)\n",
        "        return img.reshape(-1)\n",
        "\n",
        "    X_augmented = np.array([augment_image(img) for img in X])\n",
        "    y_augmented = np.tile(y_onehot, (len(X_augmented) // len(X), 1))\n",
        "    X = np.vstack((X, X_augmented))\n",
        "    y_onehot = np.vstack((y_onehot, y_augmented))\n",
        "    print(f\"Data augmented. New X shape: {X.shape}, Memory usage:\")\n",
        "    print_memory_usage()\n",
        "\n",
        "    # Split data first\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42, stratify=y_onehot)\n",
        "\n",
        "    # Standardisation basée uniquement sur X_train\n",
        "    X_train_mean = np.mean(X_train, axis=0)\n",
        "    X_train_std = np.std(X_train, axis=0)\n",
        "    X_train_std[X_train_std == 0] = 1e-10  # Éviter la division par zéro\n",
        "    X_train = (X_train - X_train_mean) / X_train_std\n",
        "    X_test = (X_test - X_train_mean) / X_train_std  # Appliquer la même transformation au test set\n",
        "    print(f\"Data standardized. Train Mean: {np.mean(X_train):.4f}, Train Std: {np.std(X_train):.4f}, Memory usage:\")\n",
        "    print_memory_usage()\n",
        "\n",
        "    print(f\"\\nForme des données d'entraînement (X_train): {X_train.shape}\")\n",
        "    print(f\"Forme des labels d'entraînement (y_train): {y_train.shape}\")\n",
        "    print(f\"Forme des données de test (X_test): {X_test.shape}\")\n",
        "    print(f\"Forme des labels de test (y_test): {y_test.shape}\")\n",
        "    print(f\"Nombre total de classes: {len(lb.classes_)}, Memory usage:\")\n",
        "    print_memory_usage()\n",
        "\n",
        "    # Visualize an example\n",
        "    plt.figure(figsize=(2, 2))\n",
        "    plt.imshow(X_train[0].reshape(32, 32), cmap='gray')\n",
        "    plt.title(f\"Label: {np.argmax(y_train[0])}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\nAucune donnée à traiter après le chargement.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daq7DvZPXMX5",
        "outputId": "1f015666-f4a5-4a74-c7b0-855f41e26f07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural Network definition complete. Memory usage:\n",
            "Current memory usage: 1390.73 MB\n"
          ]
        }
      ],
      "source": [
        "# Cell 4 - Neural Network Definition\n",
        "# Activation functions\n",
        "def relu(x):\n",
        "    \"\"\"ReLU activation: max(0, x)\"\"\"\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    \"\"\"Derivative of ReLU: 1 if x > 0, else 0\"\"\"\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Softmax activation: exp(x) / sum(exp(x))\"\"\"\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # Avoid overflow\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "# Neural Network Class\n",
        "class MultiClassNeuralNetwork:\n",
        "    def __init__(self, layer_sizes, learning_rate=0.01, l2_lambda=0.001):\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.learning_rate = learning_rate\n",
        "        self.l2_lambda = l2_lambda\n",
        "        self.weights = [np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * 0.01 for i in range(len(layer_sizes) - 1)]\n",
        "        self.biases = [np.zeros((1, layer_sizes[i + 1])) for i in range(len(layer_sizes) - 1)]\n",
        "        self.m_w = [np.zeros_like(w) for w in self.weights]  # Momentum for Adam\n",
        "        self.v_w = [np.zeros_like(w) for w in self.weights]  # RMSProp for Adam\n",
        "        self.t = 0  # Iteration counter for Adam\n",
        "        print(f\"Network initialized with layers {layer_sizes}, Memory usage:\")\n",
        "        print_memory_usage()\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.activations = [X]\n",
        "        self.z_values = []\n",
        "        for i in range(len(self.weights) - 1):\n",
        "            z = np.dot(self.activations[i], self.weights[i]) + self.biases[i]\n",
        "            self.z_values.append(z)\n",
        "            self.activations.append(relu(z))\n",
        "        z = np.dot(self.activations[-1], self.weights[-1]) + self.biases[-1]\n",
        "        self.z_values.append(z)\n",
        "        self.activations.append(softmax(z))\n",
        "        return self.activations[-1]\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        \"\"\"Categorical Cross-Entropy with L2 regularization\"\"\"\n",
        "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
        "        cross_entropy_loss = -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
        "        l2_loss = sum(np.sum(w ** 2) for w in self.weights) * self.l2_lambda / y_true.shape[0]\n",
        "        return cross_entropy_loss + l2_loss\n",
        "\n",
        "    def compute_accuracy(self, y_true, y_pred):\n",
        "        return np.mean(np.argmax(y_pred, axis=1) == np.argmax(y_true, axis=1))\n",
        "\n",
        "    def backward(self, X, y, outputs):\n",
        "        m = X.shape[0]\n",
        "        self.d_weights = [np.zeros_like(w) for w in self.weights]\n",
        "        self.d_biases = [np.zeros_like(b) for b in self.biases]\n",
        "        dZ = outputs - y\n",
        "        self.d_weights[-1] = (self.activations[-2].T @ dZ) / m + (self.l2_lambda / m) * self.weights[-1]\n",
        "        self.d_biases[-1] = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "        for i in range(len(self.weights) - 2, -1, -1):\n",
        "            dZ = (dZ @ self.weights[i + 1].T) * relu_derivative(self.z_values[i])\n",
        "            self.d_weights[i] = (self.activations[i].T @ dZ) / m + (self.l2_lambda / m) * self.weights[i]\n",
        "            self.d_biases[i] = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "\n",
        "    def update_with_adam(self):\n",
        "        self.t += 1\n",
        "        beta1, beta2, epsilon = 0.9, 0.999, 1e-8\n",
        "        self.m_w = [beta1 * m + (1 - beta1) * dw for m, dw in zip(self.m_w, self.d_weights)]\n",
        "        self.v_w = [beta2 * v + (1 - beta2) * dw**2 for v, dw in zip(self.v_w, self.d_weights)]\n",
        "        m_w_hat = [m / (1 - beta1**self.t) for m in self.m_w]\n",
        "        v_w_hat = [v / (1 - beta2**self.t) for v in self.v_w]\n",
        "        self.weights = [w - self.learning_rate * m / (np.sqrt(v) + epsilon) for w, m, v in zip(self.weights, m_w_hat, v_w_hat)]\n",
        "        self.biases = [b - self.learning_rate * db / (np.sqrt(np.sum(db**2) / db.size + epsilon)) for b, db in zip(self.biases, self.d_biases)]\n",
        "\n",
        "    def train(self, X_train, y_train, X_val, y_val, epochs, batch_size):\n",
        "        train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []\n",
        "        for epoch in range(epochs):\n",
        "            indices = np.random.permutation(X_train.shape[0])\n",
        "            X_shuffled, y_shuffled = X_train[indices], y_train[indices]\n",
        "            epoch_loss = 0\n",
        "            for i in range(0, X_train.shape[0], batch_size):\n",
        "                X_batch = X_shuffled[i:i + batch_size]\n",
        "                y_batch = y_shuffled[i:i + batch_size]\n",
        "                outputs = self.forward(X_batch)\n",
        "                epoch_loss += self.compute_loss(y_batch, outputs)\n",
        "                self.backward(X_batch, y_batch, outputs)\n",
        "                self.update_with_adam()\n",
        "            train_loss = epoch_loss / (X_train.shape[0] // batch_size)\n",
        "            train_pred = self.forward(X_train)\n",
        "            train_accuracy = self.compute_accuracy(y_train, train_pred)\n",
        "            val_pred = self.forward(X_val)\n",
        "            val_loss = self.compute_loss(y_val, val_pred)\n",
        "            val_accuracy = self.compute_accuracy(y_val, val_pred)\n",
        "            train_losses.append(train_loss)\n",
        "            val_losses.append(val_loss)\n",
        "            train_accuracies.append(train_accuracy)\n",
        "            val_accuracies.append(val_accuracy)\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
        "                      f\"Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
        "        print(f\"Training complete. Final memory usage:\")\n",
        "        print_memory_usage()\n",
        "        return train_losses, val_losses, train_accuracies, val_accuracies\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.argmax(self.forward(X), axis=1)\n",
        "\n",
        "print(\"Neural Network definition complete. Memory usage:\")\n",
        "print_memory_usage()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUs8nYYQXMEI",
        "outputId": "be5b495d-c713-402b-8985-c8a730d85bfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network initialized with layers [1024, 128, 64, 33], Memory usage:\n",
            "Current memory usage: 1519.32 MB\n",
            "Epoch 0, Train Loss: 2.2486, Val Loss: 1.9813, Train Acc: 0.4840, Val Acc: 0.4896\n",
            "Epoch 10, Train Loss: 2.7258, Val Loss: 2.0071, Train Acc: 0.5066, Val Acc: 0.5003\n",
            "Epoch 20, Train Loss: 2.6550, Val Loss: 1.8963, Train Acc: 0.5218, Val Acc: 0.5156\n",
            "Epoch 30, Train Loss: 2.7027, Val Loss: 2.0667, Train Acc: 0.4791, Val Acc: 0.4661\n",
            "Epoch 40, Train Loss: 2.5395, Val Loss: 1.9816, Train Acc: 0.5056, Val Acc: 0.4929\n",
            "Epoch 50, Train Loss: 2.1675, Val Loss: 1.6156, Train Acc: 0.6262, Val Acc: 0.6045\n",
            "Epoch 60, Train Loss: 1.8880, Val Loss: 1.4627, Train Acc: 0.6409, Val Acc: 0.6121\n",
            "Epoch 70, Train Loss: 1.7201, Val Loss: 1.3466, Train Acc: 0.6932, Val Acc: 0.6657\n",
            "Epoch 80, Train Loss: 1.5842, Val Loss: 1.1510, Train Acc: 0.7717, Val Acc: 0.7364\n",
            "Epoch 90, Train Loss: 1.4955, Val Loss: 1.1987, Train Acc: 0.7739, Val Acc: 0.7402\n",
            "Training complete. Final memory usage:\n",
            "Current memory usage: 1750.12 MB\n",
            "Fold 1 Accuracy: 0.7515\n",
            "Network initialized with layers [1024, 128, 64, 33], Memory usage:\n",
            "Current memory usage: 1637.31 MB\n",
            "Epoch 0, Train Loss: 2.3152, Val Loss: 1.9817, Train Acc: 0.5035, Val Acc: 0.5009\n",
            "Epoch 10, Train Loss: 2.6149, Val Loss: 1.9862, Train Acc: 0.5256, Val Acc: 0.5054\n",
            "Epoch 20, Train Loss: 2.6968, Val Loss: 2.2234, Train Acc: 0.4909, Val Acc: 0.4764\n",
            "Epoch 30, Train Loss: 2.7468, Val Loss: 2.0062, Train Acc: 0.5050, Val Acc: 0.4947\n",
            "Epoch 40, Train Loss: 2.6762, Val Loss: 2.0027, Train Acc: 0.4934, Val Acc: 0.4713\n",
            "Epoch 50, Train Loss: 2.6156, Val Loss: 1.9460, Train Acc: 0.5722, Val Acc: 0.5487\n",
            "Epoch 60, Train Loss: 2.5058, Val Loss: 1.9137, Train Acc: 0.5811, Val Acc: 0.5565\n",
            "Epoch 70, Train Loss: 2.1046, Val Loss: 1.4628, Train Acc: 0.6702, Val Acc: 0.6457\n",
            "Epoch 80, Train Loss: 1.7407, Val Loss: 1.1228, Train Acc: 0.7616, Val Acc: 0.7301\n",
            "Epoch 90, Train Loss: 1.5634, Val Loss: 1.0040, Train Acc: 0.7825, Val Acc: 0.7453\n",
            "Training complete. Final memory usage:\n",
            "Current memory usage: 1775.55 MB\n",
            "Fold 2 Accuracy: 0.7451\n",
            "Network initialized with layers [1024, 128, 64, 33], Memory usage:\n",
            "Current memory usage: 1775.43 MB\n",
            "Epoch 0, Train Loss: 2.1554, Val Loss: 1.9031, Train Acc: 0.5008, Val Acc: 0.4856\n",
            "Epoch 10, Train Loss: 2.8827, Val Loss: 2.4299, Train Acc: 0.4626, Val Acc: 0.4419\n",
            "Epoch 20, Train Loss: 2.9641, Val Loss: 2.2801, Train Acc: 0.4309, Val Acc: 0.4114\n",
            "Epoch 30, Train Loss: 2.9211, Val Loss: 2.2328, Train Acc: 0.4399, Val Acc: 0.4174\n",
            "Epoch 40, Train Loss: 2.9306, Val Loss: 2.3762, Train Acc: 0.4328, Val Acc: 0.4090\n",
            "Epoch 50, Train Loss: 2.9446, Val Loss: 2.2960, Train Acc: 0.4673, Val Acc: 0.4431\n",
            "Epoch 60, Train Loss: 2.9027, Val Loss: 2.2164, Train Acc: 0.4541, Val Acc: 0.4320\n",
            "Epoch 70, Train Loss: 2.8972, Val Loss: 2.3877, Train Acc: 0.4499, Val Acc: 0.4301\n",
            "Epoch 80, Train Loss: 2.8306, Val Loss: 2.3236, Train Acc: 0.4539, Val Acc: 0.4320\n",
            "Epoch 90, Train Loss: 2.8529, Val Loss: 2.1971, Train Acc: 0.4767, Val Acc: 0.4504\n",
            "Training complete. Final memory usage:\n",
            "Current memory usage: 1751.66 MB\n",
            "Fold 3 Accuracy: 0.4252\n",
            "Network initialized with layers [1024, 128, 64, 33], Memory usage:\n",
            "Current memory usage: 1783.74 MB\n",
            "Epoch 0, Train Loss: 2.1234, Val Loss: 1.8060, Train Acc: 0.5204, Val Acc: 0.5016\n",
            "Epoch 10, Train Loss: 2.0535, Val Loss: 1.6365, Train Acc: 0.6068, Val Acc: 0.5719\n",
            "Epoch 20, Train Loss: 2.0230, Val Loss: 1.5813, Train Acc: 0.6640, Val Acc: 0.6236\n",
            "Epoch 30, Train Loss: 1.9301, Val Loss: 1.4612, Train Acc: 0.6849, Val Acc: 0.6474\n",
            "Epoch 40, Train Loss: 1.8549, Val Loss: 1.5988, Train Acc: 0.7223, Val Acc: 0.6799\n",
            "Epoch 50, Train Loss: 1.7852, Val Loss: 1.3589, Train Acc: 0.7093, Val Acc: 0.6619\n",
            "Epoch 60, Train Loss: 1.5999, Val Loss: 1.1948, Train Acc: 0.7616, Val Acc: 0.7145\n",
            "Epoch 70, Train Loss: 1.4953, Val Loss: 1.1875, Train Acc: 0.7784, Val Acc: 0.7251\n",
            "Epoch 80, Train Loss: 1.4231, Val Loss: 1.0919, Train Acc: 0.7940, Val Acc: 0.7412\n",
            "Epoch 90, Train Loss: 1.4177, Val Loss: 1.0625, Train Acc: 0.8004, Val Acc: 0.7458\n"
          ]
        }
      ],
      "source": [
        "# Cell 5 - K-Fold Cross-Validation and Final Training\n",
        "if X is not None and y is not None and len(X) > 0:\n",
        "    # K-fold cross-validation\n",
        "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    fold_accuracies = []\n",
        "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train)):\n",
        "        X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n",
        "        y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
        "        nn = MultiClassNeuralNetwork(layer_sizes=[1024, 128, 64, len(lb.classes_)], learning_rate=0.01, l2_lambda=0.001)\n",
        "        train_losses, val_losses, train_accuracies, val_accuracies = nn.train(X_train_fold, y_train_fold, X_val_fold, y_val_fold, epochs=100, batch_size=32)\n",
        "        val_pred = nn.predict(X_val_fold)\n",
        "        fold_accuracy = np.mean(val_pred == np.argmax(y_val_fold, axis=1))\n",
        "        fold_accuracies.append(fold_accuracy)\n",
        "        print(f\"Fold {fold + 1} Accuracy: {fold_accuracy:.4f}\")\n",
        "\n",
        "    print(f\"Mean CV Accuracy: {np.mean(fold_accuracies):.4f} (±{np.std(fold_accuracies):.4f})\")\n",
        "\n",
        "    # Final training and evaluation\n",
        "    nn = MultiClassNeuralNetwork(layer_sizes=[1024, 64, 32, len(lb.classes_)], learning_rate=0.01, l2_lambda=0.001)\n",
        "    train_losses, val_losses, train_accuracies, val_accuracies = nn.train(X_train, y_train, X_test, y_test, epochs=100, batch_size=32)\n",
        "\n",
        "    # Plot training curves\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.title('Loss Curves')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_accuracies, label='Train Accuracy')\n",
        "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "    plt.title('Accuracy Curves')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Final evaluation\n",
        "    y_pred = nn.predict(X_test)\n",
        "    print(\"\\nClassification Report (Test set):\")\n",
        "    print(classification_report(np.argmax(y_test, axis=1), y_pred, target_names=[str(i) for i in range(len(lb.classes_))]))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(len(lb.classes_)), yticklabels=range(len(lb.classes_)))\n",
        "    plt.title('Confusion Matrix (Test set)')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\nAucune donnée à traiter après le chargement.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZ9t7ZzKUslA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}