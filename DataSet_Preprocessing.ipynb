{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Said20038/DeepLearning/blob/main/DataSet_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Multiclass Neural Network for Tifinagh Character Recognition**\n",
        "\n",
        "In this notebook, we implement a Multilayer Perceptron (MLP) designed to classify handwritten Tifinagh characters using the AMHCD dataset (28,182 images across 33 classes). The workflow includes data preprocessing, model training, hyperparameter optimization, cross-validation, and performance evaluation. Additional enhancements include L2 regularization, the Adam optimizer, K-fold cross-validation, and data augmentation techniques."
      ],
      "metadata": {
        "id": "_HUiLzn60xua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Installer les bibliothèques nécessaires\n",
        "!pip install numpy pandas opencv-python scikit-learn matplotlib seaborn gdown\n",
        "\n",
        "# Télécharger le dataset depuis Google Drive\n",
        "!gdown --id 1g03sIzi8F855KRMi0wmJesdiVQe219nY\n",
        "\n",
        "# Dézipper le fichier\n",
        "import zipfile\n",
        "with zipfile.ZipFile('amhcd-data-64.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('amhcd-data-64')\n",
        "\n",
        "# Vérifier le contenu du dossier\n",
        "print(os.listdir('amhcd-data-64'))\n",
        "\n",
        "# Fonctions d’activation\n",
        "def relu(x):\n",
        "    \"\"\"ReLU activation: max(0, x)\"\"\"\n",
        "    assert isinstance(x, np.ndarray), \"Input to ReLU must be a numpy array\"\n",
        "    result = np.maximum(0, x)\n",
        "    assert np.all(result >= 0), \"ReLU output must be non-negative\"\n",
        "    return result\n",
        "\n",
        "def relu_derivative(x):\n",
        "    \"\"\"Derivative of ReLU: 1 if x > 0, else 0\"\"\"\n",
        "    assert isinstance(x, np.ndarray), \"Input to ReLU derivative must be a numpy array\"\n",
        "    result = np.where(x > 0, 1, 0)\n",
        "    assert np.all((result == 0) | (result == 1)), \"ReLU derivative must be 0 or 1\"\n",
        "    return result\n",
        "\n",
        "def leaky_relu(x, alpha=0.01):\n",
        "    \"\"\"Leaky ReLU activation\"\"\"\n",
        "    assert isinstance(x, np.ndarray), \"Input to Leaky ReLU must be a numpy array\"\n",
        "    return np.where(x > 0, x, alpha * x)\n",
        "\n",
        "def leaky_relu_derivative(x, alpha=0.01):\n",
        "    \"\"\"Derivative of Leaky ReLU\"\"\"\n",
        "    assert isinstance(x, np.ndarray), \"Input to Leaky ReLU derivative must be a numpy array\"\n",
        "    return np.where(x > 0, 1, alpha)\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Softmax activation: exp(x) / sum(exp(x))\"\"\"\n",
        "    assert isinstance(x, np.ndarray), \"Input to softmax must be a numpy array\"\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # Éviter le débordement numérique\n",
        "    result = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "    assert np.all((result >= 0) & (result <= 1)), \"Softmax output must be in [0, 1]\"\n",
        "    assert np.allclose(np.sum(result, axis=1), 1), \"Softmax output must sum to 1 per sample\"\n",
        "    return result\n",
        "\n",
        "# Classe MultiClassNeuralNetwork\n",
        "class MultiClassNeuralNetwork:\n",
        "    def __init__(self, layer_sizes, learning_rate=0.001):\n",
        "        \"\"\"Initialize the neural network with given layer sizes and learning rate.\"\"\"\n",
        "        assert isinstance(layer_sizes, list) and len(layer_sizes) >= 2, \"layer_sizes must be a list with at least 2 elements\"\n",
        "        assert all(isinstance(size, int) and size > 0 for size in layer_sizes), \"All layer sizes must be positive integers\"\n",
        "        assert isinstance(learning_rate, (int, float)) and learning_rate > 0, \"Learning rate must be a positive number\"\n",
        "\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        self.m_w = []  # Momentum pour Adam\n",
        "        self.v_w = []  # RMSProp pour Adam\n",
        "        self.t = 0     # Compteur d'itérations pour Adam\n",
        "\n",
        "        np.random.seed(42)\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            w = np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * 0.01\n",
        "            b = np.zeros((1, layer_sizes[i + 1]))\n",
        "            self.weights.append(w)\n",
        "            self.biases.append(b)\n",
        "            self.m_w.append(np.zeros_like(w))\n",
        "            self.v_w.append(np.zeros_like(w))\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"Forward propagation.\"\"\"\n",
        "        assert isinstance(X, np.ndarray), \"Input X must be a numpy array\"\n",
        "        assert X.shape[1] == self.layer_sizes[0], f\"Input dimension ({X.shape[1]}) must match input layer size ({self.layer_sizes[0]})\"\n",
        "\n",
        "        self.activations = [X]\n",
        "        self.z_values = []\n",
        "\n",
        "        for i in range(len(self.weights) - 1):\n",
        "            z = np.dot(self.activations[i], self.weights[i]) + self.biases[i]\n",
        "            self.z_values.append(z)\n",
        "            self.activations.append(leaky_relu(z))\n",
        "\n",
        "        z = np.dot(self.activations[-1], self.weights[-1]) + self.biases[-1]\n",
        "        self.z_values.append(z)\n",
        "        output = softmax(z)\n",
        "        self.activations.append(output)\n",
        "        return self.activations[-1]\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        \"\"\"Categorical Cross-Entropy.\"\"\"\n",
        "        assert isinstance(y_true, np.ndarray) and isinstance(y_pred, np.ndarray), \"Inputs to loss must be numpy arrays\"\n",
        "        assert y_true.shape == y_pred.shape, \"y_true and y_pred must have the same shape\"\n",
        "\n",
        "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
        "        loss = -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
        "        assert not np.isnan(loss), \"Loss computation resulted in NaN\"\n",
        "        return loss\n",
        "\n",
        "    def compute_accuracy(self, y_true, y_pred):\n",
        "        \"\"\"Compute accuracy.\"\"\"\n",
        "        assert isinstance(y_true, np.ndarray) and isinstance(y_pred, np.ndarray), \"Inputs to accuracy must be numpy arrays\"\n",
        "        assert y_true.shape == y_pred.shape, \"y_true and y_pred must have the same shape\"\n",
        "\n",
        "        predictions = np.argmax(y_pred, axis=1)\n",
        "        true_labels = np.argmax(y_true, axis=1)\n",
        "        accuracy = np.mean(predictions == true_labels)\n",
        "        assert 0 <= accuracy <= 1, \"Accuracy must be between 0 and 1\"\n",
        "        return accuracy\n",
        "\n",
        "    def backward(self, X, y, outputs):\n",
        "        \"\"\"Backpropagation.\"\"\"\n",
        "        assert isinstance(X, np.ndarray) and isinstance(y, np.ndarray) and isinstance(outputs, np.ndarray), \"Inputs to backward must be numpy arrays\"\n",
        "        assert X.shape[1] == self.layer_sizes[0], f\"Input dimension ({X.shape[1]}) must match input layer size ({self.layer_sizes[0]})\"\n",
        "        assert y.shape == outputs.shape, \"y and outputs must have the same shape\"\n",
        "\n",
        "        m = X.shape[0]\n",
        "        self.d_weights = [np.zeros_like(w) for w in self.weights]\n",
        "        self.d_biases = [np.zeros_like(b) for b in self.biases]\n",
        "\n",
        "        dZ = outputs - y  # Gradient pour softmax + cross-entropy\n",
        "        self.d_weights[-1] = (self.activations[-2].T @ dZ) / m\n",
        "        self.d_biases[-1] = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "\n",
        "        for i in range(len(self.weights) - 2, -1, -1):\n",
        "            dZ = np.dot(dZ, self.weights[i + 1].T) * leaky_relu_derivative(self.z_values[i])\n",
        "            self.d_weights[i] = (self.activations[i].T @ dZ) / m\n",
        "            self.d_biases[i] = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "\n",
        "        # Régularisation L2\n",
        "        for i in range(len(self.weights)):\n",
        "            self.d_weights[i] += (0.01 / m) * self.weights[i]\n",
        "\n",
        "        self.update_with_adam()\n",
        "\n",
        "    def update_with_adam(self):\n",
        "        \"\"\"Update weights and biases using Adam optimizer.\"\"\"\n",
        "        self.t += 1\n",
        "        beta1, beta2 = 0.9, 0.999\n",
        "        epsilon = 1e-8\n",
        "        self.m_w = [beta1 * m + (1 - beta1) * dw for m, dw in zip(self.m_w, self.d_weights)]\n",
        "        self.v_w = [beta2 * v + (1 - beta2) * dw**2 for v, dw in zip(self.v_w, self.d_weights)]\n",
        "        m_w_hat = [m / (1 - beta1**self.t) for m in self.m_w]\n",
        "        v_w_hat = [v / (1 - beta2**self.t) for v in self.v_w]\n",
        "        self.weights = [w - self.learning_rate * m / (np.sqrt(v) + epsilon) for w, m, v in zip(self.weights, m_w_hat, v_w_hat)]\n",
        "        self.biases = [b - self.learning_rate * db / (np.sqrt(np.sum(db**2) / db.size + epsilon)) for b, db in zip(self.biases, self.d_biases)]\n",
        "\n",
        "    def train(self, X, y, X_val, y_val, epochs, batch_size):\n",
        "        \"\"\"Train the neural network using mini-batch SGD, with validation.\"\"\"\n",
        "        assert isinstance(X, np.ndarray) and isinstance(y, np.ndarray), \"X and y must be numpy arrays\"\n",
        "        assert isinstance(X_val, np.ndarray) and isinstance(y_val, np.ndarray), \"X_val and y_val must be numpy arrays\"\n",
        "        assert X.shape[1] == self.layer_sizes[0], f\"Input dimension ({X.shape[1]}) must match input layer size ({self.layer_sizes[0]})\"\n",
        "        assert y.shape[1] == self.layer_sizes[-1], f\"Output dimension ({y.shape[1]}) must match output layer size ({self.layer_sizes[-1]})\"\n",
        "        assert isinstance(epochs, int) and epochs > 0, \"Epochs must be a positive integer\"\n",
        "        assert isinstance(batch_size, int) and batch_size > 0, \"Batch size must be a positive integer\"\n",
        "\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        train_accuracies = []\n",
        "        val_accuracies = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            indices = np.random.permutation(X.shape[0])\n",
        "            X_shuffled = X[indices]\n",
        "            y_shuffled = y[indices]\n",
        "\n",
        "            epoch_loss = 0\n",
        "            for i in range(0, X.shape[0], batch_size):\n",
        "                X_batch = X_shuffled[i:i + batch_size]\n",
        "                y_batch = y_shuffled[i:i + batch_size]\n",
        "                outputs = self.forward(X_batch)\n",
        "                epoch_loss += self.compute_loss(y_batch, outputs)\n",
        "                self.backward(X_batch, y_batch, outputs)\n",
        "\n",
        "            train_loss = epoch_loss / (X.shape[0] // batch_size)\n",
        "            train_pred = self.forward(X)\n",
        "            train_accuracy = self.compute_accuracy(y, train_pred)\n",
        "            val_pred = self.forward(X_val)\n",
        "            val_loss = self.compute_loss(y_val, val_pred)\n",
        "            val_accuracy = self.compute_accuracy(y_val, val_pred)\n",
        "\n",
        "            train_losses.append(train_loss)\n",
        "            val_losses.append(val_loss)\n",
        "            train_accuracies.append(train_accuracy)\n",
        "            val_accuracies.append(val_accuracy)\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
        "                      f\"Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
        "\n",
        "        return train_losses, val_losses, train_accuracies, val_accuracies\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict class labels.\"\"\"\n",
        "        assert isinstance(X, np.ndarray), \"Input X must be a numpy array\"\n",
        "        assert X.shape[1] == self.layer_sizes[0], f\"Input dimension ({X.shape[1]}) must match input layer size ({self.layer_sizes[0]})\"\n",
        "\n",
        "        outputs = self.forward(X)\n",
        "        predictions = np.argmax(outputs, axis=1)\n",
        "        assert predictions.shape == (X.shape[0],), \"Predictions have incorrect shape\"\n",
        "        return predictions\n",
        "\n",
        "# Fonctions de prétraitement et augmentation\n",
        "def load_and_preprocess_image(image_path, target_size=(32, 32)):\n",
        "    \"\"\"Load and preprocess an image: convert to grayscale, resize, normalize\"\"\"\n",
        "    assert os.path.exists(image_path), f\"Image not found: {image_path}\"\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    assert img is not None, f\"Failed to load image: {image_path}\"\n",
        "    img = cv2.resize(img, target_size)\n",
        "    img = img.astype(np.float32) / 255.0  # Normalisation\n",
        "    return img\n",
        "\n",
        "def augment_image(img, target_size=(32, 32)):\n",
        "    \"\"\"Augment image with random rotation and translation.\"\"\"\n",
        "    # Random rotation\n",
        "    angle = np.random.uniform(-15, 15)\n",
        "    M = cv2.getRotationMatrix2D((16, 16), angle, 1)\n",
        "    img = cv2.warpAffine(img, M, target_size)\n",
        "    # Random translation\n",
        "    tx, ty = np.random.uniform(-3, 3, 2)\n",
        "    M = np.float32([[1, 0, tx], [0, 1, ty]])\n",
        "    img = cv2.warpAffine(img, M, target_size)\n",
        "    return img.flatten()\n",
        "\n",
        "# Chargement des données\n",
        "data_dir = os.path.join('amhcd-data-64/amhcd-data-64', 'tifinagh-images/')\n",
        "\n",
        "try:\n",
        "    labels_df = pd.read_csv(os.path.join(data_dir, 'labels-map.csv'))\n",
        "    assert 'image_path' in labels_df.columns and 'label' in labels_df.columns, \"CSV must contain 'image_path' and 'label' columns\"\n",
        "except FileNotFoundError:\n",
        "    print(\"labels-map.csv not found. Please check the dataset structure.\")\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "    for label_dir in os.listdir(data_dir):\n",
        "        label_path = os.path.join(data_dir, label_dir)\n",
        "        if os.path.isdir(label_path):\n",
        "            for img_name in os.listdir(label_path):\n",
        "                image_paths.append(os.path.join(label_path, img_name))\n",
        "                labels.append(label_dir)\n",
        "    labels_df = pd.DataFrame({'image_path': image_paths, 'label': labels})\n",
        "\n",
        "assert not labels_df.empty, \"No data loaded. Check dataset files.\"\n",
        "print(f\"Loaded {len(labels_df)} samples with {labels_df['label'].nunique()} unique classes.\")\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "labels_df['label_encoded'] = label_encoder.fit_transform(labels_df['label'])\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "# Charger et prétraiter toutes les images\n",
        "X = np.array([load_and_preprocess_image(path, (32, 32)).flatten() for path in labels_df['image_path']])\n",
        "y = labels_df['label_encoded'].values\n",
        "\n",
        "# Vérifier les dimensions\n",
        "assert X.shape[0] == y.shape[0], \"Mismatch between number of images and labels\"\n",
        "assert X.shape[1] == 32 * 32, f\"Expected flattened image size of {32*32}, got {X.shape[1]}\"\n",
        "\n",
        "# Validation croisée\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "fold_accuracies = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kfold.split(X)):\n",
        "    print(f\"Fold {fold + 1}/5\")\n",
        "    X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
        "    y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
        "    one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
        "    y_train_one_hot_fold = one_hot_encoder.fit_transform(y_train_fold.reshape(-1, 1))\n",
        "    y_val_one_hot_fold = one_hot_encoder.transform(y_val_fold.reshape(-1, 1))\n",
        "\n",
        "    # Augmenter X_train_fold\n",
        "    X_train_aug = np.array([augment_image(img.reshape(32, 32)) for img in X_train_fold])\n",
        "    y_train_aug = np.tile(y_train_fold, (len(X_train_aug) // len(X_train_fold), 1))\n",
        "    X_train_fold = np.vstack((X_train_fold, X_train_aug))\n",
        "    y_train_one_hot_fold = np.vstack((y_train_one_hot_fold, one_hot_encoder.transform(y_train_aug.reshape(-1, 1))))\n",
        "\n",
        "    # Entraîner le modèle\n",
        "    layer_sizes = [X_train_fold.shape[1], 128, 64, 16, num_classes]\n",
        "    learning_rates = [0.01, 0.001, 0.0001]\n",
        "    batch_sizes = [16, 32, 64]\n",
        "    best_accuracy = 0\n",
        "\n",
        "    for lr in learning_rates:\n",
        "        for bs in batch_sizes:\n",
        "            print(f\"Training with learning_rate={lr}, batch_size={bs}\")\n",
        "            nn = MultiClassNeuralNetwork(layer_sizes, learning_rate=lr)\n",
        "            train_losses, val_losses, train_accuracies, val_accuracies = nn.train(\n",
        "                X_train_fold, y_train_one_hot_fold, X_val_fold, y_val_one_hot_fold, epochs=100, batch_size=bs)\n",
        "            val_accuracy = val_accuracies[-1]\n",
        "            if val_accuracy > best_accuracy:\n",
        "                best_accuracy = val_accuracy\n",
        "\n",
        "    y_pred_fold = nn.predict(X_val_fold)\n",
        "    fold_accuracy = np.mean(y_pred_fold == y_val_fold)\n",
        "    fold_accuracies.append(fold_accuracy)\n",
        "    print(f\"Fold {fold + 1} Accuracy: {fold_accuracy:.4f}\")\n",
        "\n",
        "print(f\"Mean CV Accuracy: {np.mean(fold_accuracies):.4f} (±{np.std(fold_accuracies):.4f})\")\n",
        "\n",
        "# Évaluation finale sur le test set (optionnel, si nécessaire)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=42)\n",
        "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
        "y_train_one_hot = one_hot_encoder.fit_transform(y_train.reshape(-1, 1))\n",
        "y_val_one_hot = one_hot_encoder.transform(y_val.reshape(-1, 1))\n",
        "y_test_one_hot = one_hot_encoder.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "nn = MultiClassNeuralNetwork(layer_sizes, learning_rate=0.001)\n",
        "train_losses, val_losses, train_accuracies, val_accuracies = nn.train(\n",
        "    X_train, y_train_one_hot, X_val, y_val_one_hot, epochs=100, batch_size=32)\n",
        "\n",
        "y_pred = nn.predict(X_test)\n",
        "print(\"\\nRapport de classification (Test set):\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "# Matrice de confusion\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "plt.title('Matrice de confusion (Test set)')\n",
        "plt.xlabel('Prédiction')\n",
        "plt.ylabel('Réel')\n",
        "plt.savefig('confusion_matrix.png')\n",
        "plt.close()\n",
        "\n",
        "# Analyse de la matrice de confusion\n",
        "print(\"Analyse de la matrice de confusion :\")\n",
        "for i in range(len(label_encoder.classes_)):\n",
        "    row_sum = np.sum(cm[i, :])\n",
        "    if row_sum > 0:\n",
        "        accuracy_class = cm[i, i] / row_sum\n",
        "        if accuracy_class < 0.8:\n",
        "            print(f\"Classe {label_encoder.classes_[i]} : Précision = {accuracy_class:.2f} (problématique)\")\n",
        "\n",
        "# Courbes de perte et d’accuracy\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "ax1.plot(train_losses, label='Train Loss')\n",
        "ax1.plot(val_losses, label='Validation Loss')\n",
        "ax1.set_title('Courbe de perte')\n",
        "ax1.set_xlabel('Époque')\n",
        "ax1.set_ylabel('Perte')\n",
        "ax1.legend()\n",
        "\n",
        "ax2.plot(train_accuracies, label='Train Accuracy')\n",
        "ax2.plot(val_accuracies, label='Validation Accuracy')\n",
        "ax2.set_title('Courbe de précision')\n",
        "ax2.set_xlabel('Époque')\n",
        "ax2.set_ylabel('Précision')\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "fig.savefig('loss_accuracy_plot.png')\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "w__4qpXwJhdD",
        "outputId": "8920b9fc-267e-4160-f954-0c5e8696c95f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.4.26)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1g03sIzi8F855KRMi0wmJesdiVQe219nY\n",
            "From (redirected): https://drive.google.com/uc?id=1g03sIzi8F855KRMi0wmJesdiVQe219nY&confirm=t&uuid=22683e70-0130-4fc0-b36f-5791399426f0\n",
            "To: /content/amhcd-data-64.zip\n",
            "100% 30.7M/30.7M [00:00<00:00, 89.8MB/s]\n",
            "['__MACOSX', 'amhcd-data-64']\n",
            "labels-map.csv not found. Please check the dataset structure.\n",
            "Loaded 28182 samples with 33 unique classes.\n",
            "Fold 1/5\n",
            "Training with learning_rate=0.01, batch_size=16\n",
            "Epoch 0, Train Loss: 2.7007, Val Loss: 1.8796, Train Acc: 0.2980, Val Acc: 0.3562\n",
            "Epoch 10, Train Loss: 1.7569, Val Loss: 1.3622, Train Acc: 0.4432, Val Acc: 0.5404\n",
            "Epoch 20, Train Loss: 1.7242, Val Loss: 1.2137, Train Acc: 0.5007, Val Acc: 0.6042\n",
            "Epoch 30, Train Loss: 1.6888, Val Loss: 1.4092, Train Acc: 0.4540, Val Acc: 0.5491\n",
            "Epoch 40, Train Loss: 1.6869, Val Loss: 1.2065, Train Acc: 0.4957, Val Acc: 0.6161\n",
            "Epoch 50, Train Loss: 1.6773, Val Loss: 1.2582, Train Acc: 0.4891, Val Acc: 0.5994\n",
            "Epoch 60, Train Loss: 1.6400, Val Loss: 1.1406, Train Acc: 0.5036, Val Acc: 0.6108\n",
            "Epoch 70, Train Loss: 1.6614, Val Loss: 1.1781, Train Acc: 0.5079, Val Acc: 0.6103\n",
            "Epoch 80, Train Loss: 1.6469, Val Loss: 1.1907, Train Acc: 0.5027, Val Acc: 0.6221\n",
            "Epoch 90, Train Loss: 1.6440, Val Loss: 1.3185, Train Acc: 0.4727, Val Acc: 0.5824\n",
            "Training with learning_rate=0.01, batch_size=32\n",
            "Epoch 0, Train Loss: 2.2888, Val Loss: 1.4094, Train Acc: 0.4298, Val Acc: 0.5161\n",
            "Epoch 10, Train Loss: 1.2158, Val Loss: 0.7678, Train Acc: 0.6301, Val Acc: 0.7408\n",
            "Epoch 20, Train Loss: 1.1457, Val Loss: 0.7459, Train Acc: 0.6496, Val Acc: 0.7605\n",
            "Epoch 30, Train Loss: 1.1205, Val Loss: 0.6841, Train Acc: 0.6658, Val Acc: 0.7804\n",
            "Epoch 40, Train Loss: 1.0974, Val Loss: 0.7816, Train Acc: 0.6537, Val Acc: 0.7555\n",
            "Epoch 50, Train Loss: 1.0816, Val Loss: 0.7409, Train Acc: 0.6563, Val Acc: 0.7575\n",
            "Epoch 60, Train Loss: 1.0829, Val Loss: 0.7905, Train Acc: 0.6443, Val Acc: 0.7447\n",
            "Epoch 70, Train Loss: 1.0752, Val Loss: 0.8091, Train Acc: 0.6349, Val Acc: 0.7328\n",
            "Epoch 80, Train Loss: 1.0730, Val Loss: 0.6203, Train Acc: 0.6979, Val Acc: 0.7976\n",
            "Epoch 90, Train Loss: 1.0754, Val Loss: 0.6258, Train Acc: 0.6894, Val Acc: 0.7921\n",
            "Training with learning_rate=0.01, batch_size=64\n",
            "Epoch 0, Train Loss: 2.1628, Val Loss: 1.0465, Train Acc: 0.5407, Val Acc: 0.6443\n",
            "Epoch 10, Train Loss: 0.8667, Val Loss: 0.4588, Train Acc: 0.7483, Val Acc: 0.8414\n",
            "Epoch 20, Train Loss: 0.7988, Val Loss: 0.4699, Train Acc: 0.7593, Val Acc: 0.8485\n",
            "Epoch 30, Train Loss: 0.7587, Val Loss: 0.4540, Train Acc: 0.7638, Val Acc: 0.8462\n",
            "Epoch 40, Train Loss: 0.7167, Val Loss: 0.3920, Train Acc: 0.7906, Val Acc: 0.8652\n",
            "Epoch 50, Train Loss: 0.7186, Val Loss: 0.3739, Train Acc: 0.7963, Val Acc: 0.8723\n",
            "Epoch 60, Train Loss: 0.7080, Val Loss: 0.4001, Train Acc: 0.7877, Val Acc: 0.8673\n",
            "Epoch 70, Train Loss: 0.7086, Val Loss: 0.3844, Train Acc: 0.8045, Val Acc: 0.8687\n",
            "Epoch 80, Train Loss: 0.6898, Val Loss: 0.4148, Train Acc: 0.7851, Val Acc: 0.8593\n",
            "Epoch 90, Train Loss: 0.6856, Val Loss: 0.4152, Train Acc: 0.8028, Val Acc: 0.8666\n",
            "Training with learning_rate=0.001, batch_size=16\n",
            "Epoch 0, Train Loss: 3.4982, Val Loss: 3.4977, Train Acc: 0.0310, Val Acc: 0.0275\n",
            "Epoch 10, Train Loss: 3.4982, Val Loss: 3.4976, Train Acc: 0.0310, Val Acc: 0.0277\n",
            "Epoch 20, Train Loss: 3.4982, Val Loss: 3.4978, Train Acc: 0.0307, Val Acc: 0.0286\n",
            "Epoch 30, Train Loss: 3.4982, Val Loss: 3.4976, Train Acc: 0.0310, Val Acc: 0.0277\n",
            "Epoch 40, Train Loss: 3.4982, Val Loss: 3.4977, Train Acc: 0.0310, Val Acc: 0.0277\n",
            "Epoch 50, Train Loss: 2.3840, Val Loss: 1.3263, Train Acc: 0.4432, Val Acc: 0.5292\n",
            "Epoch 60, Train Loss: 0.5429, Val Loss: 0.2957, Train Acc: 0.8421, Val Acc: 0.8998\n",
            "Epoch 70, Train Loss: 0.4080, Val Loss: 0.2551, Train Acc: 0.8738, Val Acc: 0.9113\n",
            "Epoch 80, Train Loss: 0.3591, Val Loss: 0.2005, Train Acc: 0.9018, Val Acc: 0.9322\n",
            "Epoch 90, Train Loss: 0.3336, Val Loss: 0.1918, Train Acc: 0.9071, Val Acc: 0.9333\n",
            "Training with learning_rate=0.001, batch_size=32\n",
            "Epoch 0, Train Loss: 3.4993, Val Loss: 3.4974, Train Acc: 0.0310, Val Acc: 0.0277\n",
            "Epoch 10, Train Loss: 3.4993, Val Loss: 3.4976, Train Acc: 0.0310, Val Acc: 0.0277\n",
            "Epoch 20, Train Loss: 3.4993, Val Loss: 3.4977, Train Acc: 0.0310, Val Acc: 0.0277\n",
            "Epoch 30, Train Loss: 3.4993, Val Loss: 3.4976, Train Acc: 0.0310, Val Acc: 0.0277\n",
            "Epoch 40, Train Loss: 3.4993, Val Loss: 3.4977, Train Acc: 0.0310, Val Acc: 0.0277\n",
            "Epoch 50, Train Loss: 3.4993, Val Loss: 3.4977, Train Acc: 0.0310, Val Acc: 0.0277\n",
            "Epoch 60, Train Loss: 3.4992, Val Loss: 3.4979, Train Acc: 0.0307, Val Acc: 0.0286\n",
            "Epoch 70, Train Loss: 3.4993, Val Loss: 3.4977, Train Acc: 0.0310, Val Acc: 0.0273\n",
            "Epoch 80, Train Loss: 3.4993, Val Loss: 3.4979, Train Acc: 0.0310, Val Acc: 0.0273\n",
            "Epoch 90, Train Loss: 3.4993, Val Loss: 3.4979, Train Acc: 0.0311, Val Acc: 0.0271\n",
            "Training with learning_rate=0.001, batch_size=64\n",
            "Epoch 0, Train Loss: 3.0966, Val Loss: 2.6677, Train Acc: 0.1365, Val Acc: 0.1504\n",
            "Epoch 10, Train Loss: 0.7468, Val Loss: 0.4279, Train Acc: 0.7897, Val Acc: 0.8583\n",
            "Epoch 20, Train Loss: 0.4057, Val Loss: 0.2683, Train Acc: 0.8793, Val Acc: 0.9118\n",
            "Epoch 30, Train Loss: 0.2811, Val Loss: 0.2084, Train Acc: 0.9239, Val Acc: 0.9308\n",
            "Epoch 40, Train Loss: 0.2123, Val Loss: 0.2121, Train Acc: 0.9413, Val Acc: 0.9282\n",
            "Epoch 50, Train Loss: 0.1705, Val Loss: 0.2072, Train Acc: 0.9471, Val Acc: 0.9377\n",
            "Epoch 60, Train Loss: 0.1383, Val Loss: 0.1864, Train Acc: 0.9581, Val Acc: 0.9415\n",
            "Epoch 70, Train Loss: 0.1304, Val Loss: 0.2032, Train Acc: 0.9591, Val Acc: 0.9386\n",
            "Epoch 80, Train Loss: 0.1168, Val Loss: 0.2162, Train Acc: 0.9607, Val Acc: 0.9404\n",
            "Epoch 90, Train Loss: 0.1114, Val Loss: 0.2388, Train Acc: 0.9503, Val Acc: 0.9358\n",
            "Training with learning_rate=0.0001, batch_size=16\n",
            "Epoch 0, Train Loss: 3.4978, Val Loss: 3.4967, Train Acc: 0.0311, Val Acc: 0.0271\n",
            "Epoch 10, Train Loss: 3.4977, Val Loss: 3.4976, Train Acc: 0.0311, Val Acc: 0.0271\n",
            "Epoch 20, Train Loss: 3.4977, Val Loss: 3.4977, Train Acc: 0.0311, Val Acc: 0.0271\n",
            "Epoch 30, Train Loss: 3.4977, Val Loss: 3.4977, Train Acc: 0.0310, Val Acc: 0.0273\n",
            "Epoch 40, Train Loss: 3.4977, Val Loss: 3.4977, Train Acc: 0.0311, Val Acc: 0.0271\n",
            "Epoch 50, Train Loss: 3.4977, Val Loss: 3.4977, Train Acc: 0.0311, Val Acc: 0.0271\n",
            "Epoch 60, Train Loss: 3.4977, Val Loss: 3.4977, Train Acc: 0.0311, Val Acc: 0.0271\n",
            "Epoch 70, Train Loss: 3.4977, Val Loss: 3.4977, Train Acc: 0.0311, Val Acc: 0.0271\n",
            "Epoch 80, Train Loss: 3.4977, Val Loss: 3.4977, Train Acc: 0.0311, Val Acc: 0.0271\n",
            "Epoch 90, Train Loss: 3.4977, Val Loss: 3.4977, Train Acc: 0.0311, Val Acc: 0.0271\n",
            "Training with learning_rate=0.0001, batch_size=32\n",
            "Epoch 0, Train Loss: 3.2451, Val Loss: 3.0042, Train Acc: 0.0961, Val Acc: 0.1135\n",
            "Epoch 10, Train Loss: 2.8683, Val Loss: 2.6644, Train Acc: 0.1295, Val Acc: 0.1426\n",
            "Epoch 20, Train Loss: 2.4083, Val Loss: 2.1335, Train Acc: 0.2343, Val Acc: 0.2755\n",
            "Epoch 30, Train Loss: 2.0443, Val Loss: 1.7296, Train Acc: 0.3422, Val Acc: 0.4002\n",
            "Epoch 40, Train Loss: 1.7373, Val Loss: 1.4191, Train Acc: 0.4438, Val Acc: 0.5388\n",
            "Epoch 50, Train Loss: 1.4681, Val Loss: 1.1606, Train Acc: 0.5120, Val Acc: 0.6085\n",
            "Epoch 60, Train Loss: 1.2702, Val Loss: 0.9642, Train Acc: 0.5990, Val Acc: 0.6922\n",
            "Epoch 70, Train Loss: 1.1371, Val Loss: 0.8560, Train Acc: 0.6478, Val Acc: 0.7233\n",
            "Epoch 80, Train Loss: 1.0311, Val Loss: 0.7579, Train Acc: 0.6891, Val Acc: 0.7603\n",
            "Epoch 90, Train Loss: 0.9359, Val Loss: 0.6901, Train Acc: 0.7202, Val Acc: 0.7834\n",
            "Training with learning_rate=0.0001, batch_size=64\n",
            "Epoch 0, Train Loss: 3.3012, Val Loss: 3.0562, Train Acc: 0.0837, Val Acc: 0.0924\n",
            "Epoch 10, Train Loss: 2.9680, Val Loss: 2.7834, Train Acc: 0.1179, Val Acc: 0.1286\n",
            "Epoch 20, Train Loss: 2.6784, Val Loss: 2.4331, Train Acc: 0.1828, Val Acc: 0.2127\n",
            "Epoch 30, Train Loss: 2.3580, Val Loss: 2.0745, Train Acc: 0.2645, Val Acc: 0.3039\n",
            "Epoch 40, Train Loss: 2.0627, Val Loss: 1.7460, Train Acc: 0.3438, Val Acc: 0.4135\n",
            "Epoch 50, Train Loss: 1.7811, Val Loss: 1.4203, Train Acc: 0.4369, Val Acc: 0.5232\n",
            "Epoch 60, Train Loss: 1.5489, Val Loss: 1.1737, Train Acc: 0.5182, Val Acc: 0.6111\n",
            "Epoch 70, Train Loss: 1.3694, Val Loss: 0.9916, Train Acc: 0.5814, Val Acc: 0.6903\n",
            "Epoch 80, Train Loss: 1.2295, Val Loss: 0.8683, Train Acc: 0.6296, Val Acc: 0.7330\n",
            "Epoch 90, Train Loss: 1.1120, Val Loss: 0.7812, Train Acc: 0.6643, Val Acc: 0.7609\n",
            "Fold 1 Accuracy: 0.7838\n",
            "Fold 2/5\n",
            "Training with learning_rate=0.01, batch_size=16\n",
            "Epoch 0, Train Loss: 2.7862, Val Loss: 1.8963, Train Acc: 0.3175, Val Acc: 0.3905\n",
            "Epoch 10, Train Loss: 1.7490, Val Loss: 1.5251, Train Acc: 0.4073, Val Acc: 0.4926\n",
            "Epoch 20, Train Loss: 1.7166, Val Loss: 1.3207, Train Acc: 0.4678, Val Acc: 0.5655\n",
            "Epoch 30, Train Loss: 1.6975, Val Loss: 1.2638, Train Acc: 0.4584, Val Acc: 0.5537\n",
            "Epoch 40, Train Loss: 1.6863, Val Loss: 1.2953, Train Acc: 0.4822, Val Acc: 0.5757\n",
            "Epoch 50, Train Loss: 1.6683, Val Loss: 1.3457, Train Acc: 0.4601, Val Acc: 0.5498\n",
            "Epoch 60, Train Loss: 1.6765, Val Loss: 1.1721, Train Acc: 0.4950, Val Acc: 0.5969\n",
            "Epoch 70, Train Loss: 1.6767, Val Loss: 1.1802, Train Acc: 0.4949, Val Acc: 0.6111\n",
            "Epoch 80, Train Loss: 1.6704, Val Loss: 1.2214, Train Acc: 0.4879, Val Acc: 0.5833\n",
            "Epoch 90, Train Loss: 1.6779, Val Loss: 1.3107, Train Acc: 0.4715, Val Acc: 0.5746\n",
            "Training with learning_rate=0.01, batch_size=32\n",
            "Epoch 0, Train Loss: 2.1703, Val Loss: 1.3681, Train Acc: 0.4507, Val Acc: 0.5483\n",
            "Epoch 10, Train Loss: 1.2176, Val Loss: 0.7549, Train Acc: 0.6321, Val Acc: 0.7469\n",
            "Epoch 20, Train Loss: 1.1633, Val Loss: 0.8754, Train Acc: 0.6127, Val Acc: 0.7101\n",
            "Epoch 30, Train Loss: 1.1337, Val Loss: 0.7229, Train Acc: 0.6507, Val Acc: 0.7534\n",
            "Epoch 40, Train Loss: 1.1125, Val Loss: 0.6714, Train Acc: 0.6721, Val Acc: 0.7811\n",
            "Epoch 50, Train Loss: 1.0913, Val Loss: 0.7731, Train Acc: 0.6492, Val Acc: 0.7555\n",
            "Epoch 60, Train Loss: 1.0842, Val Loss: 0.7438, Train Acc: 0.6612, Val Acc: 0.7508\n",
            "Epoch 70, Train Loss: 1.0814, Val Loss: 0.6771, Train Acc: 0.6832, Val Acc: 0.7767\n",
            "Epoch 80, Train Loss: 1.0623, Val Loss: 0.6805, Train Acc: 0.6840, Val Acc: 0.7923\n",
            "Epoch 90, Train Loss: 1.0587, Val Loss: 0.6423, Train Acc: 0.6934, Val Acc: 0.7891\n",
            "Training with learning_rate=0.01, batch_size=64\n",
            "Epoch 0, Train Loss: 2.2451, Val Loss: 1.1535, Train Acc: 0.5227, Val Acc: 0.6138\n",
            "Epoch 10, Train Loss: 0.8637, Val Loss: 0.4886, Train Acc: 0.7411, Val Acc: 0.8382\n",
            "Epoch 20, Train Loss: 0.8025, Val Loss: 0.4359, Train Acc: 0.7548, Val Acc: 0.8529\n",
            "Epoch 30, Train Loss: 0.7777, Val Loss: 0.4443, Train Acc: 0.7605, Val Acc: 0.8462\n",
            "Epoch 40, Train Loss: 0.7523, Val Loss: 0.4162, Train Acc: 0.7763, Val Acc: 0.8627\n",
            "Epoch 50, Train Loss: 0.7468, Val Loss: 0.3899, Train Acc: 0.7785, Val Acc: 0.8696\n",
            "Epoch 60, Train Loss: 0.7472, Val Loss: 0.3647, Train Acc: 0.7863, Val Acc: 0.8724\n",
            "Epoch 70, Train Loss: 0.7250, Val Loss: 0.3690, Train Acc: 0.7954, Val Acc: 0.8701\n",
            "Epoch 80, Train Loss: 0.7303, Val Loss: 0.4020, Train Acc: 0.7896, Val Acc: 0.8677\n",
            "Epoch 90, Train Loss: 0.7136, Val Loss: 0.4060, Train Acc: 0.7865, Val Acc: 0.8652\n",
            "Training with learning_rate=0.001, batch_size=16\n",
            "Epoch 0, Train Loss: 3.4982, Val Loss: 3.4975, Train Acc: 0.0316, Val Acc: 0.0250\n",
            "Epoch 10, Train Loss: 3.4982, Val Loss: 3.4973, Train Acc: 0.0316, Val Acc: 0.0250\n",
            "Epoch 20, Train Loss: 3.4982, Val Loss: 3.4973, Train Acc: 0.0316, Val Acc: 0.0250\n",
            "Epoch 30, Train Loss: 3.4982, Val Loss: 3.4971, Train Acc: 0.0309, Val Acc: 0.0280\n",
            "Epoch 40, Train Loss: 3.4982, Val Loss: 3.4974, Train Acc: 0.0316, Val Acc: 0.0250\n",
            "Epoch 50, Train Loss: 3.4982, Val Loss: 3.4975, Train Acc: 0.0304, Val Acc: 0.0300\n",
            "Epoch 60, Train Loss: 3.4982, Val Loss: 3.4972, Train Acc: 0.0308, Val Acc: 0.0282\n",
            "Epoch 70, Train Loss: 3.4982, Val Loss: 3.4973, Train Acc: 0.0316, Val Acc: 0.0250\n",
            "Epoch 80, Train Loss: 3.4982, Val Loss: 3.4976, Train Acc: 0.0316, Val Acc: 0.0250\n",
            "Epoch 90, Train Loss: 3.4982, Val Loss: 3.4974, Train Acc: 0.0316, Val Acc: 0.0250\n",
            "Training with learning_rate=0.001, batch_size=32\n",
            "Epoch 0, Train Loss: 2.9274, Val Loss: 2.2557, Train Acc: 0.2045, Val Acc: 0.2397\n",
            "Epoch 10, Train Loss: 0.6548, Val Loss: 0.3393, Train Acc: 0.8169, Val Acc: 0.8897\n",
            "Epoch 20, Train Loss: 0.4002, Val Loss: 0.2274, Train Acc: 0.8863, Val Acc: 0.9234\n",
            "Epoch 30, Train Loss: 0.2914, Val Loss: 0.1923, Train Acc: 0.9136, Val Acc: 0.9365\n",
            "Epoch 40, Train Loss: 0.2367, Val Loss: 0.1646, Train Acc: 0.9359, Val Acc: 0.9470\n",
            "Epoch 50, Train Loss: 0.2124, Val Loss: 0.1730, Train Acc: 0.9430, Val Acc: 0.9420\n",
            "Epoch 60, Train Loss: 0.1937, Val Loss: 0.1951, Train Acc: 0.9236, Val Acc: 0.9328\n",
            "Epoch 70, Train Loss: 0.1794, Val Loss: 0.1795, Train Acc: 0.9412, Val Acc: 0.9431\n",
            "Epoch 80, Train Loss: 0.1709, Val Loss: 0.1573, Train Acc: 0.9507, Val Acc: 0.9477\n",
            "Epoch 90, Train Loss: 0.1691, Val Loss: 0.1429, Train Acc: 0.9550, Val Acc: 0.9507\n",
            "Training with learning_rate=0.001, batch_size=64\n",
            "Epoch 0, Train Loss: 3.0979, Val Loss: 2.7791, Train Acc: 0.1179, Val Acc: 0.1265\n",
            "Epoch 10, Train Loss: 0.7609, Val Loss: 0.4120, Train Acc: 0.7846, Val Acc: 0.8680\n",
            "Epoch 20, Train Loss: 0.4475, Val Loss: 0.2545, Train Acc: 0.8722, Val Acc: 0.9140\n",
            "Epoch 30, Train Loss: 0.3241, Val Loss: 0.2325, Train Acc: 0.8947, Val Acc: 0.9214\n",
            "Epoch 40, Train Loss: 0.2509, Val Loss: 0.2771, Train Acc: 0.9130, Val Acc: 0.9108\n",
            "Epoch 50, Train Loss: 0.2034, Val Loss: 0.2155, Train Acc: 0.9411, Val Acc: 0.9317\n",
            "Epoch 60, Train Loss: 0.1705, Val Loss: 0.2088, Train Acc: 0.9465, Val Acc: 0.9381\n",
            "Epoch 70, Train Loss: 0.1619, Val Loss: 0.2133, Train Acc: 0.9534, Val Acc: 0.9340\n",
            "Epoch 80, Train Loss: 0.1403, Val Loss: 0.2065, Train Acc: 0.9620, Val Acc: 0.9360\n",
            "Epoch 90, Train Loss: 0.1294, Val Loss: 0.2040, Train Acc: 0.9662, Val Acc: 0.9360\n",
            "Training with learning_rate=0.0001, batch_size=16\n",
            "Epoch 0, Train Loss: 3.4978, Val Loss: 3.4967, Train Acc: 0.0316, Val Acc: 0.0250\n",
            "Epoch 10, Train Loss: 3.4977, Val Loss: 3.4973, Train Acc: 0.0316, Val Acc: 0.0250\n",
            "Epoch 20, Train Loss: 3.4977, Val Loss: 3.4973, Train Acc: 0.0316, Val Acc: 0.0250\n",
            "Epoch 30, Train Loss: 3.4977, Val Loss: 3.4974, Train Acc: 0.0316, Val Acc: 0.0250\n",
            "Epoch 40, Train Loss: 3.4977, Val Loss: 3.4974, Train Acc: 0.0316, Val Acc: 0.0250\n",
            "Epoch 50, Train Loss: 3.4977, Val Loss: 3.4974, Train Acc: 0.0316, Val Acc: 0.0250\n",
            "Epoch 60, Train Loss: 3.4977, Val Loss: 3.4974, Train Acc: 0.0316, Val Acc: 0.0250\n",
            "Epoch 70, Train Loss: 3.4977, Val Loss: 3.4974, Train Acc: 0.0316, Val Acc: 0.0250\n",
            "Epoch 80, Train Loss: 3.4977, Val Loss: 3.4974, Train Acc: 0.0316, Val Acc: 0.0250\n",
            "Epoch 90, Train Loss: 3.4977, Val Loss: 3.4974, Train Acc: 0.0316, Val Acc: 0.0250\n",
            "Training with learning_rate=0.0001, batch_size=32\n",
            "Epoch 0, Train Loss: 3.2312, Val Loss: 3.0078, Train Acc: 0.0930, Val Acc: 0.0967\n",
            "Epoch 10, Train Loss: 2.8316, Val Loss: 2.6368, Train Acc: 0.1396, Val Acc: 0.1662\n",
            "Epoch 20, Train Loss: 2.3607, Val Loss: 2.0999, Train Acc: 0.2539, Val Acc: 0.3012\n",
            "Epoch 30, Train Loss: 2.0518, Val Loss: 1.7737, Train Acc: 0.3394, Val Acc: 0.4018\n",
            "Epoch 40, Train Loss: 1.7280, Val Loss: 1.4072, Train Acc: 0.4591, Val Acc: 0.5368\n",
            "Epoch 50, Train Loss: 1.4144, Val Loss: 1.0834, Train Acc: 0.5617, Val Acc: 0.6555\n",
            "Epoch 60, Train Loss: 1.1909, Val Loss: 0.8706, Train Acc: 0.6369, Val Acc: 0.7254\n",
            "Epoch 70, Train Loss: 1.0199, Val Loss: 0.7198, Train Acc: 0.6979, Val Acc: 0.7756\n",
            "Epoch 80, Train Loss: 0.8914, Val Loss: 0.6342, Train Acc: 0.7345, Val Acc: 0.8088\n",
            "Epoch 90, Train Loss: 0.7885, Val Loss: 0.5520, Train Acc: 0.7689, Val Acc: 0.8256\n",
            "Training with learning_rate=0.0001, batch_size=64\n",
            "Epoch 0, Train Loss: 3.2986, Val Loss: 3.0526, Train Acc: 0.0960, Val Acc: 0.0954\n",
            "Epoch 10, Train Loss: 2.9131, Val Loss: 2.7188, Train Acc: 0.1448, Val Acc: 0.1678\n",
            "Epoch 20, Train Loss: 2.5684, Val Loss: 2.3313, Train Acc: 0.2248, Val Acc: 0.2643\n",
            "Epoch 30, Train Loss: 2.3269, Val Loss: 2.0935, Train Acc: 0.2695, Val Acc: 0.3147\n",
            "Epoch 40, Train Loss: 2.0489, Val Loss: 1.7624, Train Acc: 0.3674, Val Acc: 0.4272\n",
            "Epoch 50, Train Loss: 1.7852, Val Loss: 1.4719, Train Acc: 0.4400, Val Acc: 0.5157\n",
            "Epoch 60, Train Loss: 1.5742, Val Loss: 1.2460, Train Acc: 0.5054, Val Acc: 0.5977\n",
            "Epoch 70, Train Loss: 1.4001, Val Loss: 1.0840, Train Acc: 0.5659, Val Acc: 0.6587\n",
            "Epoch 80, Train Loss: 1.2712, Val Loss: 0.9804, Train Acc: 0.6032, Val Acc: 0.6880\n",
            "Epoch 90, Train Loss: 1.1678, Val Loss: 0.8822, Train Acc: 0.6360, Val Acc: 0.7190\n",
            "Fold 2 Accuracy: 0.7531\n",
            "Fold 3/5\n",
            "Training with learning_rate=0.01, batch_size=16\n",
            "Epoch 0, Train Loss: 2.4516, Val Loss: 1.6728, Train Acc: 0.3703, Val Acc: 0.4539\n",
            "Epoch 10, Train Loss: 1.6310, Val Loss: 1.1489, Train Acc: 0.5034, Val Acc: 0.6038\n",
            "Epoch 20, Train Loss: 1.6057, Val Loss: 1.1773, Train Acc: 0.5015, Val Acc: 0.6081\n",
            "Epoch 30, Train Loss: 1.5871, Val Loss: 1.2298, Train Acc: 0.5058, Val Acc: 0.5951\n",
            "Epoch 40, Train Loss: 1.5833, Val Loss: 1.0438, Train Acc: 0.5505, Val Acc: 0.6560\n",
            "Epoch 50, Train Loss: 1.5761, Val Loss: 1.0729, Train Acc: 0.5393, Val Acc: 0.6460\n",
            "Epoch 60, Train Loss: 1.5769, Val Loss: 1.2093, Train Acc: 0.4980, Val Acc: 0.6097\n",
            "Epoch 70, Train Loss: 1.5763, Val Loss: 1.2494, Train Acc: 0.4961, Val Acc: 0.5896\n",
            "Epoch 80, Train Loss: 1.5668, Val Loss: 1.1405, Train Acc: 0.5245, Val Acc: 0.6192\n",
            "Epoch 90, Train Loss: 1.5794, Val Loss: 1.1397, Train Acc: 0.5176, Val Acc: 0.6299\n",
            "Training with learning_rate=0.01, batch_size=32\n",
            "Epoch 0, Train Loss: 2.2299, Val Loss: 1.3783, Train Acc: 0.4584, Val Acc: 0.5504\n",
            "Epoch 10, Train Loss: 1.2361, Val Loss: 0.8382, Train Acc: 0.6065, Val Acc: 0.7191\n",
            "Epoch 20, Train Loss: 1.2000, Val Loss: 0.7691, Train Acc: 0.6390, Val Acc: 0.7466\n",
            "Epoch 30, Train Loss: 1.1566, Val Loss: 0.8005, Train Acc: 0.6433, Val Acc: 0.7449\n",
            "Epoch 40, Train Loss: 1.1352, Val Loss: 0.7155, Train Acc: 0.6605, Val Acc: 0.7782\n",
            "Epoch 50, Train Loss: 1.1243, Val Loss: 0.6386, Train Acc: 0.6870, Val Acc: 0.7983\n",
            "Epoch 60, Train Loss: 1.1122, Val Loss: 0.7464, Train Acc: 0.6459, Val Acc: 0.7706\n",
            "Epoch 70, Train Loss: 1.1228, Val Loss: 0.7192, Train Acc: 0.6663, Val Acc: 0.7773\n",
            "Epoch 80, Train Loss: 1.1114, Val Loss: 0.6948, Train Acc: 0.6600, Val Acc: 0.7722\n",
            "Epoch 90, Train Loss: 1.1158, Val Loss: 0.6991, Train Acc: 0.6781, Val Acc: 0.7835\n",
            "Training with learning_rate=0.01, batch_size=64\n",
            "Epoch 0, Train Loss: 2.1953, Val Loss: 1.1087, Train Acc: 0.5306, Val Acc: 0.6418\n",
            "Epoch 10, Train Loss: 0.8766, Val Loss: 0.5296, Train Acc: 0.7340, Val Acc: 0.8297\n",
            "Epoch 20, Train Loss: 0.7952, Val Loss: 0.4346, Train Acc: 0.7614, Val Acc: 0.8598\n",
            "Epoch 30, Train Loss: 0.7626, Val Loss: 0.4298, Train Acc: 0.7750, Val Acc: 0.8595\n",
            "Epoch 40, Train Loss: 0.7285, Val Loss: 0.4633, Train Acc: 0.7710, Val Acc: 0.8533\n",
            "Epoch 50, Train Loss: 0.7248, Val Loss: 0.3846, Train Acc: 0.7883, Val Acc: 0.8772\n",
            "Epoch 60, Train Loss: 0.7074, Val Loss: 0.4023, Train Acc: 0.7963, Val Acc: 0.8671\n",
            "Epoch 70, Train Loss: 0.7094, Val Loss: 0.3546, Train Acc: 0.7915, Val Acc: 0.8801\n",
            "Epoch 80, Train Loss: 0.7124, Val Loss: 0.3826, Train Acc: 0.7871, Val Acc: 0.8632\n",
            "Epoch 90, Train Loss: 0.7032, Val Loss: 0.3742, Train Acc: 0.7939, Val Acc: 0.8799\n",
            "Training with learning_rate=0.001, batch_size=16\n",
            "Epoch 0, Train Loss: 3.4982, Val Loss: 3.4974, Train Acc: 0.0307, Val Acc: 0.0286\n",
            "Epoch 10, Train Loss: 1.2652, Val Loss: 0.7312, Train Acc: 0.6421, Val Acc: 0.7459\n",
            "Epoch 20, Train Loss: 0.5555, Val Loss: 0.3236, Train Acc: 0.8336, Val Acc: 0.8868\n",
            "Epoch 30, Train Loss: 0.4372, Val Loss: 0.2122, Train Acc: 0.8751, Val Acc: 0.9306\n",
            "Epoch 40, Train Loss: 0.3860, Val Loss: 0.2087, Train Acc: 0.8912, Val Acc: 0.9287\n",
            "Epoch 50, Train Loss: 0.3547, Val Loss: 0.2320, Train Acc: 0.8949, Val Acc: 0.9187\n",
            "Epoch 60, Train Loss: 0.3454, Val Loss: 0.2035, Train Acc: 0.9032, Val Acc: 0.9375\n",
            "Epoch 70, Train Loss: 0.3263, Val Loss: 0.2120, Train Acc: 0.8934, Val Acc: 0.9303\n",
            "Epoch 80, Train Loss: 0.3147, Val Loss: 0.2007, Train Acc: 0.9022, Val Acc: 0.9340\n",
            "Epoch 90, Train Loss: 0.3108, Val Loss: 0.1844, Train Acc: 0.9104, Val Acc: 0.9404\n",
            "Training with learning_rate=0.001, batch_size=32\n",
            "Epoch 0, Train Loss: 3.4993, Val Loss: 3.4972, Train Acc: 0.0313, Val Acc: 0.0264\n",
            "Epoch 10, Train Loss: 3.4993, Val Loss: 3.4975, Train Acc: 0.0313, Val Acc: 0.0264\n",
            "Epoch 20, Train Loss: 0.7845, Val Loss: 0.4292, Train Acc: 0.7717, Val Acc: 0.8616\n",
            "Epoch 30, Train Loss: 0.4524, Val Loss: 0.2276, Train Acc: 0.8772, Val Acc: 0.9281\n",
            "Epoch 40, Train Loss: 0.3429, Val Loss: 0.2267, Train Acc: 0.8941, Val Acc: 0.9242\n",
            "Epoch 50, Train Loss: 0.2895, Val Loss: 0.1946, Train Acc: 0.9232, Val Acc: 0.9393\n",
            "Epoch 60, Train Loss: 0.2520, Val Loss: 0.1738, Train Acc: 0.9317, Val Acc: 0.9469\n",
            "Epoch 70, Train Loss: 0.2286, Val Loss: 0.1689, Train Acc: 0.9316, Val Acc: 0.9443\n",
            "Epoch 80, Train Loss: 0.2082, Val Loss: 0.1823, Train Acc: 0.9366, Val Acc: 0.9386\n",
            "Epoch 90, Train Loss: 0.2007, Val Loss: 0.1739, Train Acc: 0.9446, Val Acc: 0.9414\n",
            "Training with learning_rate=0.001, batch_size=64\n",
            "Epoch 0, Train Loss: 3.0758, Val Loss: 2.6697, Train Acc: 0.1440, Val Acc: 0.1703\n",
            "Epoch 10, Train Loss: 0.8190, Val Loss: 0.4392, Train Acc: 0.7760, Val Acc: 0.8666\n",
            "Epoch 20, Train Loss: 0.4872, Val Loss: 0.2818, Train Acc: 0.8528, Val Acc: 0.9022\n",
            "Epoch 30, Train Loss: 0.3355, Val Loss: 0.2184, Train Acc: 0.9071, Val Acc: 0.9329\n",
            "Epoch 40, Train Loss: 0.2617, Val Loss: 0.2201, Train Acc: 0.9162, Val Acc: 0.9246\n",
            "Epoch 50, Train Loss: 0.2202, Val Loss: 0.2220, Train Acc: 0.9383, Val Acc: 0.9294\n",
            "Epoch 60, Train Loss: 0.1855, Val Loss: 0.2237, Train Acc: 0.9395, Val Acc: 0.9301\n",
            "Epoch 70, Train Loss: 0.1579, Val Loss: 0.2058, Train Acc: 0.9509, Val Acc: 0.9365\n",
            "Epoch 80, Train Loss: 0.1391, Val Loss: 0.2207, Train Acc: 0.9523, Val Acc: 0.9363\n",
            "Epoch 90, Train Loss: 0.1277, Val Loss: 0.1791, Train Acc: 0.9696, Val Acc: 0.9445\n",
            "Training with learning_rate=0.0001, batch_size=16\n",
            "Epoch 0, Train Loss: 3.4978, Val Loss: 3.4967, Train Acc: 0.0313, Val Acc: 0.0264\n",
            "Epoch 10, Train Loss: 3.4977, Val Loss: 3.4973, Train Acc: 0.0313, Val Acc: 0.0264\n",
            "Epoch 20, Train Loss: 3.4977, Val Loss: 3.4974, Train Acc: 0.0313, Val Acc: 0.0264\n",
            "Epoch 30, Train Loss: 3.4977, Val Loss: 3.4974, Train Acc: 0.0313, Val Acc: 0.0264\n",
            "Epoch 40, Train Loss: 3.4977, Val Loss: 3.4974, Train Acc: 0.0313, Val Acc: 0.0264\n",
            "Epoch 50, Train Loss: 3.4977, Val Loss: 3.4974, Train Acc: 0.0313, Val Acc: 0.0264\n",
            "Epoch 60, Train Loss: 3.4977, Val Loss: 3.4974, Train Acc: 0.0313, Val Acc: 0.0264\n",
            "Epoch 70, Train Loss: 3.4977, Val Loss: 3.4974, Train Acc: 0.0313, Val Acc: 0.0264\n",
            "Epoch 80, Train Loss: 3.4977, Val Loss: 3.4974, Train Acc: 0.0313, Val Acc: 0.0264\n",
            "Epoch 90, Train Loss: 3.4977, Val Loss: 3.4974, Train Acc: 0.0313, Val Acc: 0.0264\n",
            "Training with learning_rate=0.0001, batch_size=32\n",
            "Epoch 0, Train Loss: 3.2288, Val Loss: 3.0042, Train Acc: 0.0977, Val Acc: 0.1031\n",
            "Epoch 10, Train Loss: 2.8107, Val Loss: 2.6347, Train Acc: 0.1437, Val Acc: 0.1675\n",
            "Epoch 20, Train Loss: 2.3992, Val Loss: 2.1405, Train Acc: 0.2497, Val Acc: 0.2903\n",
            "Epoch 30, Train Loss: 2.1127, Val Loss: 1.8304, Train Acc: 0.3215, Val Acc: 0.3836\n",
            "Epoch 40, Train Loss: 1.7540, Val Loss: 1.4066, Train Acc: 0.4532, Val Acc: 0.5371\n",
            "Epoch 50, Train Loss: 1.4664, Val Loss: 1.1098, Train Acc: 0.5412, Val Acc: 0.6260\n",
            "Epoch 60, Train Loss: 1.2767, Val Loss: 0.9322, Train Acc: 0.6104, Val Acc: 0.6962\n",
            "Epoch 70, Train Loss: 1.1273, Val Loss: 0.7975, Train Acc: 0.6625, Val Acc: 0.7482\n",
            "Epoch 80, Train Loss: 1.0087, Val Loss: 0.7354, Train Acc: 0.6943, Val Acc: 0.7651\n",
            "Epoch 90, Train Loss: 0.9067, Val Loss: 0.6600, Train Acc: 0.7273, Val Acc: 0.7901\n",
            "Training with learning_rate=0.0001, batch_size=64\n",
            "Epoch 0, Train Loss: 3.2952, Val Loss: 3.0555, Train Acc: 0.0923, Val Acc: 0.0979\n",
            "Epoch 10, Train Loss: 2.9675, Val Loss: 2.8245, Train Acc: 0.1219, Val Acc: 0.1357\n",
            "Epoch 20, Train Loss: 2.5891, Val Loss: 2.3661, Train Acc: 0.2099, Val Acc: 0.2376\n",
            "Epoch 30, Train Loss: 2.3561, Val Loss: 2.1069, Train Acc: 0.2643, Val Acc: 0.3082\n",
            "Epoch 40, Train Loss: 2.1190, Val Loss: 1.8373, Train Acc: 0.3338, Val Acc: 0.3879\n",
            "Epoch 50, Train Loss: 1.9037, Val Loss: 1.5921, Train Acc: 0.3882, Val Acc: 0.4562\n",
            "Epoch 60, Train Loss: 1.7008, Val Loss: 1.3625, Train Acc: 0.4598, Val Acc: 0.5444\n",
            "Epoch 70, Train Loss: 1.4889, Val Loss: 1.1374, Train Acc: 0.5398, Val Acc: 0.6340\n",
            "Epoch 80, Train Loss: 1.3368, Val Loss: 0.9768, Train Acc: 0.5854, Val Acc: 0.6925\n",
            "Epoch 90, Train Loss: 1.2160, Val Loss: 0.8602, Train Acc: 0.6309, Val Acc: 0.7353\n",
            "Fold 3 Accuracy: 0.7429\n",
            "Fold 4/5\n",
            "Training with learning_rate=0.01, batch_size=16\n",
            "Epoch 0, Train Loss: 2.4798, Val Loss: 1.6961, Train Acc: 0.3562, Val Acc: 0.4184\n",
            "Epoch 10, Train Loss: 1.6079, Val Loss: 1.1370, Train Acc: 0.5098, Val Acc: 0.6065\n",
            "Epoch 20, Train Loss: 1.5893, Val Loss: 1.3569, Train Acc: 0.4828, Val Acc: 0.5665\n",
            "Epoch 30, Train Loss: 1.5817, Val Loss: 1.2455, Train Acc: 0.5019, Val Acc: 0.6036\n",
            "Epoch 40, Train Loss: 1.5816, Val Loss: 1.0843, Train Acc: 0.5349, Val Acc: 0.6428\n",
            "Epoch 50, Train Loss: 1.5632, Val Loss: 1.0789, Train Acc: 0.5397, Val Acc: 0.6492\n",
            "Epoch 60, Train Loss: 1.5781, Val Loss: 1.2125, Train Acc: 0.5103, Val Acc: 0.6127\n",
            "Epoch 70, Train Loss: 1.5804, Val Loss: 1.1234, Train Acc: 0.5418, Val Acc: 0.6320\n",
            "Epoch 80, Train Loss: 1.5753, Val Loss: 1.1207, Train Acc: 0.5285, Val Acc: 0.6315\n",
            "Epoch 90, Train Loss: 1.5719, Val Loss: 1.2095, Train Acc: 0.5163, Val Acc: 0.6097\n",
            "Training with learning_rate=0.01, batch_size=32\n",
            "Epoch 0, Train Loss: 2.1493, Val Loss: 1.2946, Train Acc: 0.4796, Val Acc: 0.5774\n",
            "Epoch 10, Train Loss: 1.1732, Val Loss: 0.7837, Train Acc: 0.6300, Val Acc: 0.7346\n",
            "Epoch 20, Train Loss: 1.1208, Val Loss: 0.7896, Train Acc: 0.6533, Val Acc: 0.7488\n",
            "Epoch 30, Train Loss: 1.1036, Val Loss: 0.7749, Train Acc: 0.6512, Val Acc: 0.7590\n",
            "Epoch 40, Train Loss: 1.0936, Val Loss: 0.7484, Train Acc: 0.6492, Val Acc: 0.7424\n",
            "Epoch 50, Train Loss: 1.0889, Val Loss: 0.6689, Train Acc: 0.6810, Val Acc: 0.7846\n",
            "Epoch 60, Train Loss: 1.0859, Val Loss: 0.7142, Train Acc: 0.6793, Val Acc: 0.7677\n",
            "Epoch 70, Train Loss: 1.0752, Val Loss: 0.6423, Train Acc: 0.6897, Val Acc: 0.7809\n",
            "Epoch 80, Train Loss: 1.0801, Val Loss: 0.6882, Train Acc: 0.6762, Val Acc: 0.7779\n",
            "Epoch 90, Train Loss: 1.0769, Val Loss: 0.6875, Train Acc: 0.6703, Val Acc: 0.7796\n",
            "Training with learning_rate=0.01, batch_size=64\n",
            "Epoch 0, Train Loss: 2.1560, Val Loss: 1.1352, Train Acc: 0.5092, Val Acc: 0.6036\n",
            "Epoch 10, Train Loss: 0.8371, Val Loss: 0.4361, Train Acc: 0.7621, Val Acc: 0.8570\n",
            "Epoch 20, Train Loss: 0.7628, Val Loss: 0.4435, Train Acc: 0.7699, Val Acc: 0.8520\n",
            "Epoch 30, Train Loss: 0.7346, Val Loss: 0.4543, Train Acc: 0.7639, Val Acc: 0.8492\n",
            "Epoch 40, Train Loss: 0.7204, Val Loss: 0.4831, Train Acc: 0.7687, Val Acc: 0.8410\n",
            "Epoch 50, Train Loss: 0.7151, Val Loss: 0.4028, Train Acc: 0.7939, Val Acc: 0.8707\n",
            "Epoch 60, Train Loss: 0.7161, Val Loss: 0.3667, Train Acc: 0.7949, Val Acc: 0.8740\n",
            "Epoch 70, Train Loss: 0.7055, Val Loss: 0.4502, Train Acc: 0.7806, Val Acc: 0.8501\n",
            "Epoch 80, Train Loss: 0.7010, Val Loss: 0.4159, Train Acc: 0.7880, Val Acc: 0.8644\n",
            "Epoch 90, Train Loss: 0.6939, Val Loss: 0.3864, Train Acc: 0.7998, Val Acc: 0.8715\n",
            "Training with learning_rate=0.001, batch_size=16\n",
            "Epoch 0, Train Loss: 3.4982, Val Loss: 3.4977, Train Acc: 0.0313, Val Acc: 0.0263\n",
            "Epoch 10, Train Loss: 3.4982, Val Loss: 3.4979, Train Acc: 0.0308, Val Acc: 0.0284\n",
            "Epoch 20, Train Loss: 3.4982, Val Loss: 3.4982, Train Acc: 0.0313, Val Acc: 0.0263\n",
            "Epoch 30, Train Loss: 3.4982, Val Loss: 3.4982, Train Acc: 0.0313, Val Acc: 0.0263\n",
            "Epoch 40, Train Loss: 1.0434, Val Loss: 0.6082, Train Acc: 0.7120, Val Acc: 0.7961\n",
            "Epoch 50, Train Loss: 0.4977, Val Loss: 0.2407, Train Acc: 0.8718, Val Acc: 0.9248\n",
            "Epoch 60, Train Loss: 0.4028, Val Loss: 0.2339, Train Acc: 0.8879, Val Acc: 0.9168\n",
            "Epoch 70, Train Loss: 0.3601, Val Loss: 0.2097, Train Acc: 0.8915, Val Acc: 0.9296\n",
            "Epoch 80, Train Loss: 0.3371, Val Loss: 0.2000, Train Acc: 0.9033, Val Acc: 0.9342\n",
            "Epoch 90, Train Loss: 0.3220, Val Loss: 0.2071, Train Acc: 0.9084, Val Acc: 0.9292\n",
            "Training with learning_rate=0.001, batch_size=32\n",
            "Epoch 0, Train Loss: 2.8671, Val Loss: 2.0883, Train Acc: 0.2528, Val Acc: 0.2931\n",
            "Epoch 10, Train Loss: 0.6021, Val Loss: 0.3722, Train Acc: 0.8079, Val Acc: 0.8694\n",
            "Epoch 20, Train Loss: 0.3746, Val Loss: 0.2407, Train Acc: 0.8821, Val Acc: 0.9171\n",
            "Epoch 30, Train Loss: 0.2859, Val Loss: 0.1868, Train Acc: 0.9197, Val Acc: 0.9375\n",
            "Epoch 40, Train Loss: 0.2345, Val Loss: 0.1776, Train Acc: 0.9377, Val Acc: 0.9420\n",
            "Epoch 50, Train Loss: 0.2067, Val Loss: 0.2009, Train Acc: 0.9235, Val Acc: 0.9303\n",
            "Epoch 60, Train Loss: 0.1932, Val Loss: 0.1736, Train Acc: 0.9375, Val Acc: 0.9411\n",
            "Epoch 70, Train Loss: 0.1845, Val Loss: 0.1436, Train Acc: 0.9504, Val Acc: 0.9501\n",
            "Epoch 80, Train Loss: 0.1728, Val Loss: 0.1281, Train Acc: 0.9593, Val Acc: 0.9574\n",
            "Epoch 90, Train Loss: 0.1644, Val Loss: 0.1459, Train Acc: 0.9544, Val Acc: 0.9523\n",
            "Training with learning_rate=0.001, batch_size=64\n",
            "Epoch 0, Train Loss: 3.0741, Val Loss: 2.6075, Train Acc: 0.1453, Val Acc: 0.1618\n",
            "Epoch 10, Train Loss: 0.6895, Val Loss: 0.3965, Train Acc: 0.8075, Val Acc: 0.8726\n",
            "Epoch 20, Train Loss: 0.4095, Val Loss: 0.2907, Train Acc: 0.8779, Val Acc: 0.9070\n",
            "Epoch 30, Train Loss: 0.2925, Val Loss: 0.2475, Train Acc: 0.9143, Val Acc: 0.9214\n",
            "Epoch 40, Train Loss: 0.2252, Val Loss: 0.2565, Train Acc: 0.9162, Val Acc: 0.9184\n",
            "Epoch 50, Train Loss: 0.1782, Val Loss: 0.2180, Train Acc: 0.9473, Val Acc: 0.9313\n",
            "Epoch 60, Train Loss: 0.1485, Val Loss: 0.2067, Train Acc: 0.9615, Val Acc: 0.9313\n",
            "Epoch 70, Train Loss: 0.1328, Val Loss: 0.2216, Train Acc: 0.9586, Val Acc: 0.9328\n",
            "Epoch 80, Train Loss: 0.1217, Val Loss: 0.2113, Train Acc: 0.9654, Val Acc: 0.9379\n",
            "Epoch 90, Train Loss: 0.1149, Val Loss: 0.2359, Train Acc: 0.9630, Val Acc: 0.9329\n",
            "Training with learning_rate=0.0001, batch_size=16\n",
            "Epoch 0, Train Loss: 3.4978, Val Loss: 3.4967, Train Acc: 0.0314, Val Acc: 0.0261\n",
            "Epoch 10, Train Loss: 3.4977, Val Loss: 3.4976, Train Acc: 0.0314, Val Acc: 0.0261\n",
            "Epoch 20, Train Loss: 3.4977, Val Loss: 3.4977, Train Acc: 0.0313, Val Acc: 0.0263\n",
            "Epoch 30, Train Loss: 3.4977, Val Loss: 3.4977, Train Acc: 0.0314, Val Acc: 0.0261\n",
            "Epoch 40, Train Loss: 3.4977, Val Loss: 3.4978, Train Acc: 0.0313, Val Acc: 0.0263\n",
            "Epoch 50, Train Loss: 3.4977, Val Loss: 3.4978, Train Acc: 0.0314, Val Acc: 0.0261\n",
            "Epoch 60, Train Loss: 3.4977, Val Loss: 3.4978, Train Acc: 0.0314, Val Acc: 0.0261\n",
            "Epoch 70, Train Loss: 3.4977, Val Loss: 3.4978, Train Acc: 0.0314, Val Acc: 0.0261\n",
            "Epoch 80, Train Loss: 3.4977, Val Loss: 3.4978, Train Acc: 0.0313, Val Acc: 0.0263\n",
            "Epoch 90, Train Loss: 3.4977, Val Loss: 3.4978, Train Acc: 0.0314, Val Acc: 0.0261\n",
            "Training with learning_rate=0.0001, batch_size=32\n",
            "Epoch 0, Train Loss: 3.2404, Val Loss: 2.9956, Train Acc: 0.0899, Val Acc: 0.0992\n",
            "Epoch 10, Train Loss: 2.8296, Val Loss: 2.6202, Train Acc: 0.1508, Val Acc: 0.1670\n",
            "Epoch 20, Train Loss: 2.3120, Val Loss: 2.0675, Train Acc: 0.2643, Val Acc: 0.3039\n",
            "Epoch 30, Train Loss: 2.0039, Val Loss: 1.7313, Train Acc: 0.3619, Val Acc: 0.4186\n",
            "Epoch 40, Train Loss: 1.7499, Val Loss: 1.4512, Train Acc: 0.4413, Val Acc: 0.5160\n",
            "Epoch 50, Train Loss: 1.4812, Val Loss: 1.1526, Train Acc: 0.5379, Val Acc: 0.6230\n",
            "Epoch 60, Train Loss: 1.2914, Val Loss: 0.9789, Train Acc: 0.5992, Val Acc: 0.6780\n",
            "Epoch 70, Train Loss: 1.1349, Val Loss: 0.8610, Train Acc: 0.6511, Val Acc: 0.7276\n",
            "Epoch 80, Train Loss: 1.0206, Val Loss: 0.7572, Train Acc: 0.6939, Val Acc: 0.7576\n",
            "Epoch 90, Train Loss: 0.9308, Val Loss: 0.6984, Train Acc: 0.7189, Val Acc: 0.7720\n",
            "Training with learning_rate=0.0001, batch_size=64\n",
            "Epoch 0, Train Loss: 3.2986, Val Loss: 3.0414, Train Acc: 0.0914, Val Acc: 0.1034\n",
            "Epoch 10, Train Loss: 2.9451, Val Loss: 2.7729, Train Acc: 0.1266, Val Acc: 0.1377\n",
            "Epoch 20, Train Loss: 2.5600, Val Loss: 2.3132, Train Acc: 0.2133, Val Acc: 0.2502\n",
            "Epoch 30, Train Loss: 2.2726, Val Loss: 2.0057, Train Acc: 0.2867, Val Acc: 0.3380\n",
            "Epoch 40, Train Loss: 1.9965, Val Loss: 1.7045, Train Acc: 0.3698, Val Acc: 0.4335\n",
            "Epoch 50, Train Loss: 1.7395, Val Loss: 1.4244, Train Acc: 0.4477, Val Acc: 0.5319\n",
            "Epoch 60, Train Loss: 1.5524, Val Loss: 1.2198, Train Acc: 0.5139, Val Acc: 0.5962\n",
            "Epoch 70, Train Loss: 1.3996, Val Loss: 1.0687, Train Acc: 0.5700, Val Acc: 0.6627\n",
            "Epoch 80, Train Loss: 1.2679, Val Loss: 0.9457, Train Acc: 0.6098, Val Acc: 0.7000\n",
            "Epoch 90, Train Loss: 1.1536, Val Loss: 0.8424, Train Acc: 0.6488, Val Acc: 0.7399\n",
            "Fold 4 Accuracy: 0.7496\n",
            "Fold 5/5\n",
            "Training with learning_rate=0.01, batch_size=16\n",
            "Epoch 0, Train Loss: 3.4949, Val Loss: 3.2477, Train Acc: 0.0618, Val Acc: 0.0672\n",
            "Epoch 10, Train Loss: 1.8817, Val Loss: 1.4765, Train Acc: 0.4257, Val Acc: 0.5188\n",
            "Epoch 20, Train Loss: 1.8378, Val Loss: 1.4087, Train Acc: 0.4332, Val Acc: 0.5378\n",
            "Epoch 30, Train Loss: 1.8096, Val Loss: 1.4748, Train Acc: 0.4262, Val Acc: 0.5282\n",
            "Epoch 40, Train Loss: 1.7724, Val Loss: 1.3679, Train Acc: 0.4652, Val Acc: 0.5587\n",
            "Epoch 50, Train Loss: 1.7612, Val Loss: 1.3572, Train Acc: 0.4463, Val Acc: 0.5561\n",
            "Epoch 60, Train Loss: 1.7705, Val Loss: 1.4337, Train Acc: 0.4371, Val Acc: 0.5438\n",
            "Epoch 70, Train Loss: 1.7662, Val Loss: 1.3610, Train Acc: 0.4627, Val Acc: 0.5527\n",
            "Epoch 80, Train Loss: 1.7445, Val Loss: 1.3260, Train Acc: 0.4702, Val Acc: 0.5749\n",
            "Epoch 90, Train Loss: 1.7265, Val Loss: 1.3993, Train Acc: 0.4507, Val Acc: 0.5461\n",
            "Training with learning_rate=0.01, batch_size=32\n",
            "Epoch 0, Train Loss: 2.2063, Val Loss: 1.2806, Train Acc: 0.4844, Val Acc: 0.5841\n",
            "Epoch 10, Train Loss: 1.1882, Val Loss: 0.8074, Train Acc: 0.6394, Val Acc: 0.7441\n",
            "Epoch 20, Train Loss: 1.1351, Val Loss: 0.7484, Train Acc: 0.6654, Val Acc: 0.7718\n",
            "Epoch 30, Train Loss: 1.1048, Val Loss: 0.6938, Train Acc: 0.6599, Val Acc: 0.7738\n",
            "Epoch 40, Train Loss: 1.0987, Val Loss: 0.7542, Train Acc: 0.6623, Val Acc: 0.7590\n",
            "Epoch 50, Train Loss: 1.0832, Val Loss: 0.6751, Train Acc: 0.6855, Val Acc: 0.7766\n",
            "Epoch 60, Train Loss: 1.0880, Val Loss: 0.6952, Train Acc: 0.6750, Val Acc: 0.7763\n",
            "Epoch 70, Train Loss: 1.0852, Val Loss: 0.6880, Train Acc: 0.6873, Val Acc: 0.7823\n",
            "Epoch 80, Train Loss: 1.0845, Val Loss: 0.7488, Train Acc: 0.6734, Val Acc: 0.7580\n",
            "Epoch 90, Train Loss: 1.0754, Val Loss: 0.7252, Train Acc: 0.6719, Val Acc: 0.7695\n",
            "Training with learning_rate=0.01, batch_size=64\n",
            "Epoch 0, Train Loss: 2.2254, Val Loss: 1.1241, Train Acc: 0.5124, Val Acc: 0.6137\n",
            "Epoch 10, Train Loss: 0.8762, Val Loss: 0.4630, Train Acc: 0.7404, Val Acc: 0.8405\n",
            "Epoch 20, Train Loss: 0.8045, Val Loss: 0.4320, Train Acc: 0.7679, Val Acc: 0.8607\n",
            "Epoch 30, Train Loss: 0.7878, Val Loss: 0.4498, Train Acc: 0.7704, Val Acc: 0.8499\n",
            "Epoch 40, Train Loss: 0.7625, Val Loss: 0.4092, Train Acc: 0.7885, Val Acc: 0.8689\n",
            "Epoch 50, Train Loss: 0.7349, Val Loss: 0.4665, Train Acc: 0.7673, Val Acc: 0.8437\n",
            "Epoch 60, Train Loss: 0.7254, Val Loss: 0.3823, Train Acc: 0.7948, Val Acc: 0.8708\n",
            "Epoch 70, Train Loss: 0.7131, Val Loss: 0.4553, Train Acc: 0.7718, Val Acc: 0.8469\n",
            "Epoch 80, Train Loss: 0.7095, Val Loss: 0.4787, Train Acc: 0.7742, Val Acc: 0.8345\n",
            "Epoch 90, Train Loss: 0.7106, Val Loss: 0.3745, Train Acc: 0.8014, Val Acc: 0.8776\n",
            "Training with learning_rate=0.001, batch_size=16\n",
            "Epoch 0, Train Loss: 3.4982, Val Loss: 3.4975, Train Acc: 0.0313, Val Acc: 0.0263\n",
            "Epoch 10, Train Loss: 0.8625, Val Loss: 0.4842, Train Acc: 0.7585, Val Acc: 0.8469\n",
            "Epoch 20, Train Loss: 0.4733, Val Loss: 0.2638, Train Acc: 0.8659, Val Acc: 0.9191\n",
            "Epoch 30, Train Loss: 0.3886, Val Loss: 0.2577, Train Acc: 0.8779, Val Acc: 0.9154\n",
            "Epoch 40, Train Loss: 0.3538, Val Loss: 0.2295, Train Acc: 0.8896, Val Acc: 0.9264\n",
            "Epoch 50, Train Loss: 0.3244, Val Loss: 0.2475, Train Acc: 0.8931, Val Acc: 0.9145\n",
            "Epoch 60, Train Loss: 0.3109, Val Loss: 0.1799, Train Acc: 0.9121, Val Acc: 0.9411\n",
            "Epoch 70, Train Loss: 0.3055, Val Loss: 0.2392, Train Acc: 0.8973, Val Acc: 0.9196\n",
            "Epoch 80, Train Loss: 0.2942, Val Loss: 0.2259, Train Acc: 0.9044, Val Acc: 0.9237\n",
            "Epoch 90, Train Loss: 0.2903, Val Loss: 0.1671, Train Acc: 0.9273, Val Acc: 0.9441\n",
            "Training with learning_rate=0.001, batch_size=32\n",
            "Epoch 0, Train Loss: 2.8936, Val Loss: 2.1413, Train Acc: 0.2543, Val Acc: 0.2917\n",
            "Epoch 10, Train Loss: 0.5620, Val Loss: 0.3003, Train Acc: 0.8422, Val Acc: 0.9029\n",
            "Epoch 20, Train Loss: 0.3478, Val Loss: 0.2339, Train Acc: 0.8979, Val Acc: 0.9212\n",
            "Epoch 30, Train Loss: 0.2687, Val Loss: 0.1878, Train Acc: 0.9201, Val Acc: 0.9377\n",
            "Epoch 40, Train Loss: 0.2312, Val Loss: 0.1908, Train Acc: 0.9243, Val Acc: 0.9342\n",
            "Epoch 50, Train Loss: 0.2020, Val Loss: 0.2019, Train Acc: 0.9243, Val Acc: 0.9345\n",
            "Epoch 60, Train Loss: 0.1839, Val Loss: 0.1602, Train Acc: 0.9483, Val Acc: 0.9496\n",
            "Epoch 70, Train Loss: 0.1757, Val Loss: 0.1470, Train Acc: 0.9553, Val Acc: 0.9537\n",
            "Epoch 80, Train Loss: 0.1661, Val Loss: 0.1575, Train Acc: 0.9509, Val Acc: 0.9468\n",
            "Epoch 90, Train Loss: 0.1607, Val Loss: 0.1396, Train Acc: 0.9574, Val Acc: 0.9532\n",
            "Training with learning_rate=0.001, batch_size=64\n",
            "Epoch 0, Train Loss: 3.0859, Val Loss: 2.6296, Train Acc: 0.1499, Val Acc: 0.1762\n",
            "Epoch 10, Train Loss: 0.7335, Val Loss: 0.4109, Train Acc: 0.7983, Val Acc: 0.8708\n",
            "Epoch 20, Train Loss: 0.4313, Val Loss: 0.2827, Train Acc: 0.8720, Val Acc: 0.9104\n",
            "Epoch 30, Train Loss: 0.3155, Val Loss: 0.2240, Train Acc: 0.9167, Val Acc: 0.9228\n",
            "Epoch 40, Train Loss: 0.2418, Val Loss: 0.2121, Train Acc: 0.9378, Val Acc: 0.9304\n",
            "Epoch 50, Train Loss: 0.1954, Val Loss: 0.2046, Train Acc: 0.9486, Val Acc: 0.9338\n",
            "Epoch 60, Train Loss: 0.1729, Val Loss: 0.2288, Train Acc: 0.9518, Val Acc: 0.9235\n",
            "Epoch 70, Train Loss: 0.1454, Val Loss: 0.2148, Train Acc: 0.9533, Val Acc: 0.9352\n",
            "Epoch 80, Train Loss: 0.1452, Val Loss: 0.2357, Train Acc: 0.9533, Val Acc: 0.9308\n",
            "Epoch 90, Train Loss: 0.1147, Val Loss: 0.2674, Train Acc: 0.9484, Val Acc: 0.9292\n",
            "Training with learning_rate=0.0001, batch_size=16\n",
            "Epoch 0, Train Loss: 3.4978, Val Loss: 3.4967, Train Acc: 0.0313, Val Acc: 0.0263\n",
            "Epoch 10, Train Loss: 3.4977, Val Loss: 3.4975, Train Acc: 0.0313, Val Acc: 0.0263\n",
            "Epoch 20, Train Loss: 3.4977, Val Loss: 3.4977, Train Acc: 0.0313, Val Acc: 0.0263\n",
            "Epoch 30, Train Loss: 3.4977, Val Loss: 3.4977, Train Acc: 0.0313, Val Acc: 0.0263\n",
            "Epoch 40, Train Loss: 3.4977, Val Loss: 3.4977, Train Acc: 0.0313, Val Acc: 0.0263\n",
            "Epoch 50, Train Loss: 3.4977, Val Loss: 3.4977, Train Acc: 0.0313, Val Acc: 0.0263\n",
            "Epoch 60, Train Loss: 3.4977, Val Loss: 3.4977, Train Acc: 0.0313, Val Acc: 0.0263\n",
            "Epoch 70, Train Loss: 3.4977, Val Loss: 3.4977, Train Acc: 0.0313, Val Acc: 0.0263\n",
            "Epoch 80, Train Loss: 3.4977, Val Loss: 3.4977, Train Acc: 0.0312, Val Acc: 0.0266\n",
            "Epoch 90, Train Loss: 3.4977, Val Loss: 3.4977, Train Acc: 0.0313, Val Acc: 0.0263\n",
            "Training with learning_rate=0.0001, batch_size=32\n",
            "Epoch 0, Train Loss: 3.2457, Val Loss: 2.9923, Train Acc: 0.0936, Val Acc: 0.1010\n",
            "Epoch 10, Train Loss: 2.8138, Val Loss: 2.6233, Train Acc: 0.1496, Val Acc: 0.1696\n",
            "Epoch 20, Train Loss: 2.3385, Val Loss: 2.0749, Train Acc: 0.2598, Val Acc: 0.3031\n",
            "Epoch 30, Train Loss: 1.9579, Val Loss: 1.6533, Train Acc: 0.3775, Val Acc: 0.4381\n",
            "Epoch 40, Train Loss: 1.6106, Val Loss: 1.2957, Train Acc: 0.4971, Val Acc: 0.5775\n",
            "Epoch 50, Train Loss: 1.3572, Val Loss: 1.0487, Train Acc: 0.5789, Val Acc: 0.6648\n",
            "Epoch 60, Train Loss: 1.1813, Val Loss: 0.8972, Train Acc: 0.6372, Val Acc: 0.7250\n",
            "Epoch 70, Train Loss: 1.0519, Val Loss: 0.7799, Train Acc: 0.6823, Val Acc: 0.7590\n",
            "Epoch 80, Train Loss: 0.9448, Val Loss: 0.7043, Train Acc: 0.7180, Val Acc: 0.7828\n",
            "Epoch 90, Train Loss: 0.8517, Val Loss: 0.6499, Train Acc: 0.7427, Val Acc: 0.7921\n",
            "Training with learning_rate=0.0001, batch_size=64\n",
            "Epoch 0, Train Loss: 3.2998, Val Loss: 3.0429, Train Acc: 0.0959, Val Acc: 0.1054\n",
            "Epoch 10, Train Loss: 2.9251, Val Loss: 2.7537, Train Acc: 0.1314, Val Acc: 0.1521\n",
            "Epoch 20, Train Loss: 2.5709, Val Loss: 2.3335, Train Acc: 0.2132, Val Acc: 0.2436\n",
            "Epoch 30, Train Loss: 2.3371, Val Loss: 2.0853, Train Acc: 0.2597, Val Acc: 0.2999\n",
            "Epoch 40, Train Loss: 2.0773, Val Loss: 1.7912, Train Acc: 0.3415, Val Acc: 0.4108\n",
            "Epoch 50, Train Loss: 1.7932, Val Loss: 1.4727, Train Acc: 0.4306, Val Acc: 0.5167\n",
            "Epoch 60, Train Loss: 1.5991, Val Loss: 1.2722, Train Acc: 0.4989, Val Acc: 0.5946\n",
            "Epoch 70, Train Loss: 1.4541, Val Loss: 1.1332, Train Acc: 0.5504, Val Acc: 0.6370\n",
            "Epoch 80, Train Loss: 1.3201, Val Loss: 1.0180, Train Acc: 0.5921, Val Acc: 0.6776\n",
            "Epoch 90, Train Loss: 1.2063, Val Loss: 0.9123, Train Acc: 0.6360, Val Acc: 0.7156\n",
            "Fold 5 Accuracy: 0.7244\n",
            "Mean CV Accuracy: 0.7508 (±0.0192)\n",
            "Epoch 0, Train Loss: 3.0450, Val Loss: 2.8027, Train Acc: 0.1226, Val Acc: 0.1180\n",
            "Epoch 10, Train Loss: 0.5065, Val Loss: 0.7165, Train Acc: 0.8458, Val Acc: 0.7736\n",
            "Epoch 20, Train Loss: 0.2028, Val Loss: 0.5347, Train Acc: 0.9380, Val Acc: 0.8403\n",
            "Epoch 30, Train Loss: 0.1225, Val Loss: 0.4463, Train Acc: 0.9655, Val Acc: 0.8730\n",
            "Epoch 40, Train Loss: 0.0835, Val Loss: 0.4166, Train Acc: 0.9753, Val Acc: 0.8866\n",
            "Epoch 50, Train Loss: 0.0962, Val Loss: 0.4159, Train Acc: 0.9775, Val Acc: 0.8937\n",
            "Epoch 60, Train Loss: 0.0652, Val Loss: 0.3030, Train Acc: 0.9931, Val Acc: 0.9104\n",
            "Epoch 70, Train Loss: 0.1025, Val Loss: 0.2712, Train Acc: 0.9944, Val Acc: 0.9214\n",
            "Epoch 80, Train Loss: 0.0500, Val Loss: 0.2792, Train Acc: 0.9914, Val Acc: 0.9170\n",
            "Epoch 90, Train Loss: 0.0463, Val Loss: 0.3640, Train Acc: 0.9754, Val Acc: 0.9014\n",
            "\n",
            "Rapport de classification (Test set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          ya       0.90      0.96      0.93       171\n",
            "         yab       0.84      0.92      0.88       171\n",
            "        yach       0.96      0.87      0.91       171\n",
            "         yad       0.87      0.94      0.90       171\n",
            "        yadd       0.87      0.87      0.87       171\n",
            "         yae       0.94      0.97      0.96       171\n",
            "         yaf       0.94      0.88      0.91       171\n",
            "         yag       0.97      0.89      0.93       171\n",
            "        yagg       0.99      0.96      0.98       170\n",
            "        yagh       0.96      0.95      0.95       170\n",
            "         yah       0.95      0.91      0.93       171\n",
            "        yahh       0.98      0.82      0.89       171\n",
            "         yaj       0.92      0.90      0.91       171\n",
            "         yak       0.93      0.85      0.89       171\n",
            "        yakk       0.98      0.98      0.98       171\n",
            "         yal       0.87      0.94      0.91       170\n",
            "         yam       0.95      0.96      0.96       171\n",
            "         yan       0.99      0.86      0.92       170\n",
            "         yaq       0.89      0.96      0.92       171\n",
            "         yar       0.77      0.94      0.85       171\n",
            "        yarr       0.92      0.86      0.89       171\n",
            "         yas       0.90      0.87      0.88       171\n",
            "        yass       0.95      0.92      0.93       171\n",
            "         yat       0.93      0.98      0.96       171\n",
            "        yatt       0.79      0.87      0.82       171\n",
            "         yaw       0.94      0.94      0.94       171\n",
            "         yax       0.98      0.91      0.95       170\n",
            "         yay       0.94      0.91      0.92       171\n",
            "         yaz       0.93      0.90      0.91       171\n",
            "        yazz       0.88      0.95      0.91       171\n",
            "         yey       0.81      0.96      0.88       171\n",
            "          yi       0.90      0.88      0.89       170\n",
            "          yu       0.92      0.84      0.88       171\n",
            "\n",
            "    accuracy                           0.91      5637\n",
            "   macro avg       0.92      0.91      0.91      5637\n",
            "weighted avg       0.92      0.91      0.91      5637\n",
            "\n",
            "Analyse de la matrice de confusion :\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}